{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af540b5-b0cf-4a3e-8b94-e2c38ac531e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikeb\\.conda\\envs\\sandbox\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a321295-3413-43f9-a205-894673015490",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('mlburnham/Pol_NLI')\n",
    "test = ds['test'].to_pandas()\n",
    "ndocs = 5000\n",
    "test = test.sample(ndocs, random_state = 1)\n",
    "timings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59c4f8-fce9-4465-b6f5-f20788dc1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_it(pipe, docs, n=1):\n",
    "    \"\"\"\n",
    "    Benchmark the time taken by a model pipeline.\n",
    "\n",
    "    Args:\n",
    "        pipe (callable): The model pipeline to benchmark.\n",
    "        docs (list): A list of documents that will be passed to the pipe.\n",
    "        n (int): Number of times to run the benchmark. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the model name, hardware, average time, DPS (documents per second),\n",
    "              and standard errors for both metrics if n > 1.\n",
    "    \"\"\"\n",
    "    times = []\n",
    "\n",
    "    for i in range(n):\n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "        results = pipe(docs, 'This text is about politics.', hypothesis_template='{}')\n",
    "        # Stop timer\n",
    "        end_time = time.time()\n",
    "        # Calculate the elapsed time for this run\n",
    "        elapsed_time = end_time - start_time\n",
    "        times.append(elapsed_time)\n",
    "\n",
    "        print(f\"Run {i + 1}/{n} - Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Calculate the average time and DPS\n",
    "    avg_time = np.mean(times)\n",
    "    dps = ndocs / avg_time\n",
    "\n",
    "    # Calculate standard errors if n > 1\n",
    "    if n > 1:\n",
    "        time_se = np.std(times, ddof=1) / np.sqrt(n)\n",
    "        dps_se = (np.std([ndocs / t for t in times], ddof=1) / np.sqrt(n))\n",
    "    else:\n",
    "        time_se = None\n",
    "        dps_se = None\n",
    "\n",
    "    print(f\"Average elapsed time: {avg_time:.2f} seconds\")\n",
    "    print(f\"Average DPS: {dps}\")\n",
    "\n",
    "    if n > 1:\n",
    "        print(f\"Standard error (Time): {time_se:.4f} seconds\")\n",
    "        print(f\"Standard error (DPS): {dps_se:.4f}\")\n",
    "\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "    res = {\n",
    "        'Model': model.split('/')[-1],\n",
    "        'Hardware': pipe.device.type,\n",
    "        'Time': avg_time,\n",
    "        'Time_SE': time_se,\n",
    "        'DPS': dps,\n",
    "        'DPS_SE': dps_se\n",
    "    }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf363b21-135f-4c5d-acb0-1b18368ee73d",
   "metadata": {},
   "source": [
    "# DEBATE Base DeBERTa RTX 3090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e3e3b2-159b-43ef-8ba9-c5bc7cca9425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_base_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = 'cuda', batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95180514-4be7-4df4-b71d-3be564994382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once to compile\n",
    "time_it(pipe, n = 1)\n",
    "# benchmark\n",
    "res = time_it(pipe, n = 10)\n",
    "timings.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee676f3e-e269-4a00-aeb8-065daefbaee2",
   "metadata": {},
   "source": [
    "# DEBATE Base DeBERTa Ryzen 9900x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2ad45aa-bccf-4a78-bc34-5a405061f8e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_large_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = 'cpu', batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc22d0b-42b7-44c9-ad28-c7b2bdf154fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once to compile\n",
    "time_it(pipe, n = 1)\n",
    "# benchmark\n",
    "res = time_it(pipe, n = 10)\n",
    "timings.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f81df86-f3b0-4eb3-a0d7-6bb78d168186",
   "metadata": {},
   "source": [
    "# DEBATE Large DeBERTa RTX 3090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d73b7b-5688-4460-b528-b79802f46316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_large_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = 'cuda', batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51937fcb-2d31-448c-9c00-0a8ccd133ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once to compile\n",
    "time_it(pipe, n = 1)\n",
    "# benchmark\n",
    "res = time_it(pipe, n = 10)\n",
    "timings.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9c2922-ba88-43db-b362-1f633407de7a",
   "metadata": {},
   "source": [
    "# DEBATE Large DeBERTa Ryzen 9900x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46243d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_large_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = 'cpu', batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d70d4e-63fa-405e-9806-437d57ed7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once to compile\n",
    "time_it(pipe, n = 1)\n",
    "# benchmark\n",
    "res = time_it(pipe, n = 10)\n",
    "timings.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41ad38-c470-44ff-9b12-03f599f3844f",
   "metadata": {},
   "source": [
    "# DEBATE Large Modern BERT RTX 3090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a288e25-abe9-4e56-b1d1-80c186f32ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_ModernBERT_large_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = 'cuda', batch_size = 8, torch_dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f078b6-b959-4228-b020-81e724e421ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once to compile\n",
    "time_it(pipe, n = 1)\n",
    "# benchmark\n",
    "res = time_it(pipe, n = 10)\n",
    "timings.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec04b6-9add-4434-a18e-69b37e5a5807",
   "metadata": {},
   "source": [
    "# DEBATE Large Modern BERT Ryzen 9900x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91552ff5-fd78-4754-9430-d8b45aec56b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_ModernBERT_large_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = 'cpu', batch_size = 8, torch_dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6f4b7-3917-487a-a8a4-c7672ecc772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once to compile\n",
    "time_it(pipe, n = 1)\n",
    "# benchmark\n",
    "res = time_it(pipe, n = 10)\n",
    "timings.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5783b6-4636-44d0-8771-e2d487ed63d6",
   "metadata": {},
   "source": [
    "# DEBATE Base Modern BERT RTX 3090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d607b-cb86-45ed-a1c0-62349472e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_ModernBERT_base_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = 'cuda', batch_size = 8, torch_dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a89ed-11f6-4f27-928a-95b69b03c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once to compile\n",
    "time_it(pipe, n = 1)\n",
    "# benchmark\n",
    "res = time_it(pipe, n = 10)\n",
    "timings.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe11ef-1cda-474f-a087-d6a5332aba7f",
   "metadata": {},
   "source": [
    "# DEBATE Base Modern BERT Ryzen 9900x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe652e4-732a-4fbe-ad47-719ea452f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_ModernBERT_base_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = 'cpu', batch_size = 8, torch_dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78e10a4-b84e-4ba0-8d12-9d52ff5f7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once to compile\n",
    "time_it(pipe, n = 1)\n",
    "# benchmark\n",
    "res = time_it(pipe, n = 10)\n",
    "timings.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ffb8c6-3be0-4bf5-b4e8-75707765288a",
   "metadata": {},
   "source": [
    "# Llama-3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d3ab5ed-43ad-4be5-b8b6-87c8e5b4b4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22c7ddea1b842c5ac8b5ebb33f83529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "pipe = pipeline(\"text-generation\", model=model, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map='cuda', batch_size = 1,\n",
    "token = \"########\")\n",
    "pipe.tokenizer.pad_token_id = pipe.model.config.eos_token_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66f5b318-63ca-4ae3-93f1-b5ec598ab3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"You are a classifier that can only respond with 1 or 0. I'm going to show you a short text sample and I want you to determine if this text is about politics. Here is the text:\n",
    "{doc}\n",
    "\n",
    "If it is true that this text is about politics, return 1. If it is not true that this text is about politics, return 0.\n",
    "Do not explain your answer, and only return 1 or 0.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70b6e4aa-36bc-451e-ba92-5da0c9fb96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": user_message.format(doc = doc)} for doc in test['premise']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3ab02fd-1395-4864-a85a-7161b0b08e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [pipe.tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True) for message in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28cf68d5-7c19-45a3-926b-0d5d0050cdc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2219.90 seconds\n",
      "DPS: 2.252349324300326\n"
     ]
    }
   ],
   "source": [
    "torch.mps.empty_cache()\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "results = pipe(prompt, max_new_tokens=2, do_sample=False, return_full_text = False, pad_token_id=pipe.tokenizer.pad_token_id, temperature = 0)\n",
    "# Stop timer\n",
    "end_time = time.time()\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"DPS: {ndocs/elapsed_time}\")\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "timings.append({\n",
    "                'Model': model.split('/')[-1],\n",
    "                'Hardware': 'cuda',\n",
    "                'Time': elapsed_time,\n",
    "                'DPS': ndocs/elapsed_time\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa1356-0ed5-4da9-b79a-c6fc130461ab",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bb1c650-e3eb-48b9-b302-1abdbf745c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps = pd.DataFrame(timings)\n",
    "mps.to_csv('../data/cuda_timing.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
