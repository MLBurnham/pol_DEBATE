{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20450cad-44f1-482f-ab1a-711adbfb9f53",
   "metadata": {},
   "source": [
    "- rename models in df\n",
    "- rename tasks in df\n",
    "- print table for tasks, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c40f6477-ecc5-46b4-a5d0-c8c4d032a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a7897bc-97d3-4ebe-a23c-fa94b0e881d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_table(df, metric=\"F1\", caption=\"\", label=\"\", output_format=\"latex\"):\n",
    "    \"\"\"\n",
    "    Generate a table from a pandas DataFrame containing F1 scores and standard errors.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with columns 'Model', 'Task', 'F1', and 'F1_SE'.\n",
    "    metric (str, optional): The metric to use (default is \"F1\").\n",
    "    caption (str, optional): Caption for the table. Defaults to \"F1 macro scores with standard errors for `out of distribution' tasks.\".\n",
    "    label (str, optional): Label for the table. Defaults to \"tab:ood_bench\".\n",
    "    output_format (str, optional): Format of the table. Use \"latex\" for LaTeX output and \"markdown\" for Markdown output.\n",
    "    \n",
    "    Returns:\n",
    "    str: A formatted table as a string in the specified format.\n",
    "    \"\"\"\n",
    "    # Define the tasks based on the DataFrame\n",
    "    tasks = df['Task'].unique()\n",
    "    \n",
    "    # Pivot the DataFrame so that rows = Model and columns = Task.\n",
    "    pivot_ = df.pivot(index='Model', columns='Task', values=f'{metric}')\n",
    "    pivot_SE = df.pivot(index='Model', columns='Task', values=f'{metric}_SE')\n",
    "    \n",
    "    # Compute best scores for each task\n",
    "    best_scores = {task: pivot_[task].max() if task in pivot_.columns else None for task in tasks}\n",
    "    \n",
    "    # Get unique models\n",
    "    models = df[\"Model\"].unique()\n",
    "    \n",
    "    if output_format.lower() == \"latex\":\n",
    "        # Begin constructing the LaTeX table as a list of strings.\n",
    "        latex_lines = [\n",
    "            r\"\\begin{table}[htbp]\",\n",
    "            r\"\\centering\",\n",
    "            fr'\\caption{{{caption}}}',\n",
    "            fr\"\\label{{{label}}}\",\n",
    "            r\"\\begin{tabular}{p{3cm}\" + \"p{.9cm}\" * len(tasks) + r\"}\",\n",
    "            r\"\\toprule\",\n",
    "            \"Model & \" + \" & \".join(tasks) + r\" \\\\\",\n",
    "            r\"\\toprule\"\n",
    "        ]\n",
    "        \n",
    "        for model in models:\n",
    "            row_cells = []\n",
    "            for task in tasks:\n",
    "                if task in pivot_.columns:\n",
    "                    score = pivot_.loc[model, task]\n",
    "                    se = pivot_SE.loc[model, task]\n",
    "                    if pd.notna(score):\n",
    "                        # Bold the best score\n",
    "                        if best_scores[task] is not None and score == best_scores[task]:\n",
    "                            cell = rf\"$\\mathbf{{{100*score:.1f}}}_{{ {100*se:.1f} }}$\"\n",
    "                        else:\n",
    "                            cell = rf\"${100*score:.1f}_{{ {100*se:.1f} }}$\"\n",
    "                    else:\n",
    "                        cell = \"\"\n",
    "                else:\n",
    "                    cell = \"\"\n",
    "                row_cells.append(cell)\n",
    "            \n",
    "            # Add row to LaTeX table\n",
    "            row_str = model + \" & \" + \" & \".join(row_cells) + r\" \\\\[1ex]\"\n",
    "            latex_lines.append(row_str)\n",
    "        \n",
    "        # Finalize table\n",
    "        latex_lines.extend([r\"\\bottomrule\", r\"\\end{tabular}\", r\"\\end{table}\"])\n",
    "        return \"\\n\".join(latex_lines)\n",
    "    \n",
    "    elif output_format.lower() == \"markdown\":\n",
    "        markdown_lines = []\n",
    "        if caption:\n",
    "            markdown_lines.append(f\"**{caption}**\")\n",
    "            markdown_lines.append(\"\")  # blank line for spacing\n",
    "        \n",
    "        # Header row\n",
    "        header_row = \"Model | \" + \" | \".join(tasks)\n",
    "        markdown_lines.append(header_row)\n",
    "        \n",
    "        # Separator row (one cell for Model and one for each task)\n",
    "        separator_row = \" | \".join([\"---\"] * (len(tasks) + 1))\n",
    "        markdown_lines.append(separator_row)\n",
    "        \n",
    "        for model in models:\n",
    "            row_cells = [model]\n",
    "            for task in tasks:\n",
    "                if task in pivot_.columns:\n",
    "                    score = pivot_.loc[model, task]\n",
    "                    se = pivot_SE.loc[model, task]\n",
    "                    if pd.notna(score):\n",
    "                        if best_scores[task] is not None and score == best_scores[task]:\n",
    "                            cell = f\"**{100*score:.1f} ({100*se:.1f})**\"\n",
    "                        else:\n",
    "                            cell = f\"{100*score:.1f} ({100*se:.1f})\"\n",
    "                    else:\n",
    "                        cell = \"\"\n",
    "                else:\n",
    "                    cell = \"\"\n",
    "                row_cells.append(cell)\n",
    "            markdown_lines.append(\" | \".join(row_cells))\n",
    "        \n",
    "        return \"\\n\".join(markdown_lines)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported output_format. Use 'latex' or 'markdown'.\")\n",
    "\n",
    "def sort_df(df, colname, order):\n",
    "    \"\"\"\n",
    "    df: The dataframe to sort\n",
    "    colname: The column to sort by\n",
    "    order: The order in which values are to appear\n",
    "    \"\"\"\n",
    "    return df.sort_values(by=colname, key=lambda column: column.map(lambda e: order.index(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28be00a9-12be-4dfe-8151-79db05bda6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/results_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e5ed04f-132c-4b25-b3e7-039e4102ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "modnames = {\n",
    "    'base_nli': 'DeBERTa_Base', \n",
    "    'large_nli': 'DeBERTa_Large',\n",
    "    'base_debate': 'DEBATE_Base',\n",
    "    'large_debate': 'DEBATE_Large',\n",
    "    'base_modern': 'DEBATE_Base(MB)',\n",
    "    'large_modern': 'DEBATE_Large(MB)',\n",
    "    'llama': 'Llama 3.1_8B',\n",
    "    'sonnet': 'Claude 3.5'\n",
    "}\n",
    "df['Model'] = df['Model'].replace(modnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34bbecfb-38e2-4282-8bda-9c52348b8b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasknames = {\n",
    "    'overall': 'Overall',\n",
    "    'event extraction': 'Event',\n",
    "    'hatespeech and toxicity': 'Hatespeech',\n",
    "    'stance detection': 'Stance',\n",
    "    'topic classification': 'Topic',\n",
    "    'PoliStance_Affect': 'Polistance Affect',\n",
    "    'PoliStance_Affect_QT': 'Polistance Affect Qt',\n",
    "    'acled_event_entailment': 'ACLED',\n",
    "    'argument_quality_ranking_entailment': 'Argument Quality Ranking',\n",
    "    'bill_summary_entailment': 'Bill Summary',\n",
    "    'dehumanizing_hatespeech_entailment': 'Dehumanizing Hatespeech',\n",
    "    'dem_rep_party_platform_topics': 'Party Platforms',\n",
    "    'ibm_claimstance_entailment': 'Claimstance',\n",
    "    'ibm_claimstance_topic_entailment': 'Claimstance Topic',\n",
    "    'polistance_issue_tweets': 'Polistance Issue Tweets',\n",
    "    'scad_event_entailment': 'SCAD',\n",
    "    'targeted_hatespeech_entailment': 'Targeted Hatespeech',\n",
    "    'violent_hatespeech_entailment': 'Violent Hatespeech'\n",
    "}\n",
    "df['Task'] = df['Task'].replace(tasknames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8da0317c-4a0c-4d2c-a587-427b2feecad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Model | Stance | Topic | Hatespeech | Event | Overall\n",
       "--- | --- | --- | --- | --- | ---\n",
       "Llama 3.1_8B | 80.0 (0.6) | 96.0 (0.3) | 79.9 (0.7) | **90.5 (0.5)** | 86.3 (0.3)\n",
       "DEBATE_Large(MB) | 96.7 (0.3) | 97.1 (0.2) | 95.5 (0.4) | 88.1 (0.6) | 95.0 (0.2)\n",
       "DEBATE_Base(MB) | 94.9 (0.3) | 96.4 (0.3) | 94.1 (0.4) | 83.7 (0.7) | 93.1 (0.2)\n",
       "DEBATE_Large | **98.1 (0.2)** | **97.6 (0.2)** | **95.7 (0.4)** | 90.4 (0.5) | **96.1 (0.2)**\n",
       "Claude 3.5 | 89.8 (0.4) | 97.4 (0.2) | 85.4 (0.7) | 88.1 (0.6) | 90.9 (0.2)\n",
       "DeBERTa_Base | 80.5 (0.6) | 92.0 (0.4) | 83.6 (0.8) | 74.7 (0.8) | 83.5 (0.3)\n",
       "DEBATE_Base | 97.0 (0.2) | 96.9 (0.3) | 95.2 (0.4) | 86.3 (0.7) | 94.6 (0.2)\n",
       "DeBERTa_Large | 81.3 (0.6) | 94.1 (0.4) | 86.0 (0.7) | 86.0 (0.7) | 87.0 (0.3)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasks = ['Stance', 'Topic', 'Hatespeech', 'Event', 'Overall']\n",
    "task = df[df['Task'].isin(tasks)]\n",
    "task = sort_df(task, 'Task', tasks)\n",
    "\n",
    "task_md = generate_table(task, output_format = 'markdown')\n",
    "display(Markdown(task_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60483c84-5adc-4119-bd2e-e50ba91b4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_data = []\n",
    "stance = df[df['Task'].isin(stance_data)]\n",
    "stance = sort_df(stance, 'Task', stance_data)\n",
    "\n",
    "stance_md = generate_table(stance, output_format = 'markdown')\n",
    "display(Markdown(stance_md))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
