{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import torch # pipeline will claim to be using mps w/o this but torch must be imported otherwise it falls back to cpu\n",
    "# Make the sure accelerate library is installed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "2.2.2\n",
      "True\n",
      "True\n",
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('mlburnham/Pol_NLI')\n",
    "test = ds['test'].to_pandas()\n",
    "ndocs = 5000\n",
    "test = test.sample(ndocs, random_state = 1)\n",
    "timings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3 Max Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_base_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = torch.device(\"mps\"), batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 88.14 seconds\n",
      "DPS: 56.727489455970705\n",
      "Elapsed time: 88.14 seconds\n",
      "DPS: 56.727489455970705\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "results = pipe(list(test['premise']), 'This text is about politics.', hypothesis_template='{}', multi_label=False)\n",
    "# Stop timer\n",
    "end_time = time.time()\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "dps = ndocs/elapsed_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"DPS: {dps}\")\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "timings.append({\n",
    "                'Model': model.split('/')[-1],\n",
    "                'Hardware': 'mps',\n",
    "                'Time': elapsed_time,\n",
    "                'DPS': ndocs/elapsed_time\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3 Max Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_large_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = torch.device(\"mps\"), batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 215.29 seconds\n",
      "Elapsed time: 215.29 seconds\n"
     ]
    }
   ],
   "source": [
    "torch.mps.empty_cache()\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "results = pipe(list(test['premise']), 'This text is about politics.', hypothesis_template='{}', multi_label=False)\n",
    "# Stop timer\n",
    "end_time = time.time()\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"DPS: {ndocs/elapsed_time}\")\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "results.append({\n",
    "                'Model': model.split('/')[-1],\n",
    "                'Hardware': 'mps',\n",
    "                'Time': elapsed_time,\n",
    "                'DPS': ndocs/elapsed_time\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rLoading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rLoading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rLoading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rLoading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "pipe = pipeline(\"text-generation\", model=model, model_kwargs={\"torch_dtype\": torch.float16}, device_map='mps', batch_size = 1,\n",
    "token = \"########\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"You are a classifier that can only respond with 1 or 0. I'm going to show you a short text sample and I want you to determine if this text is about politics. Here is the text:\n",
    "{doc}\n",
    "\n",
    "If it is true that this text is about politics, return 1. If it is not true that this text is about politics, return 0.\n",
    "Do not explain your answer, and only return 1 or 0.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": user_message.format(doc = doc)} for doc in test['premise']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = [pipe.tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True) for message in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mb7336/miniforge3/envs/sandbox/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/mb7336/miniforge3/envs/sandbox/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2026.35 seconds\n",
      "DPS: 2.4674868670061945\n",
      "Elapsed time: 2026.35 seconds\n",
      "DPS: 2.4674868670061945\n"
     ]
    }
   ],
   "source": [
    "torch.mps.empty_cache()\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "results = pipe(prompt, max_new_tokens=2, do_sample=False, return_full_text = False, pad_token_id=pipe.tokenizer.eos_token_id, temperature = 0)\n",
    "# Stop timer\n",
    "end_time = time.time()\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"DPS: {ndocs/elapsed_time}\")\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "results.append({\n",
    "                'Model': model.split('/')[-1],\n",
    "                'Hardware': 'mps',\n",
    "                'Time': elapsed_time,\n",
    "                'DPS': ndocs/elapsed_time\n",
    "            })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
