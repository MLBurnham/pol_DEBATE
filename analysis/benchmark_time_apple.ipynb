{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import numpy as np\n",
    "import torch # pipeline will claim to be using mps w/o this but torch must be imported otherwise it falls back to cpu\n",
    "# Make the sure accelerate library is installed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('mlburnham/Pol_NLI')\n",
    "test = ds['test'].to_pandas()\n",
    "ndocs = 5000\n",
    "test = test.sample(ndocs, random_state = 1)\n",
    "timings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_it(pipe, docs, n=1):\n",
    "    \"\"\"\n",
    "    Benchmark the time taken by a model pipeline.\n",
    "\n",
    "    Args:\n",
    "        pipe (callable): The model pipeline to benchmark.\n",
    "        docs (list): A list of documents that will be passed to the pipe.\n",
    "        n (int): Number of times to run the benchmark. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the model name, hardware, average time, DPS (documents per second),\n",
    "              and standard errors for both metrics if n > 1.\n",
    "    \"\"\"\n",
    "    times = []\n",
    "\n",
    "    for i in range(n):\n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "        results = pipe(docs, 'This text is about politics.', hypothesis_template='{}')\n",
    "        # Stop timer\n",
    "        end_time = time.time()\n",
    "        # Calculate the elapsed time for this run\n",
    "        elapsed_time = end_time - start_time\n",
    "        times.append(elapsed_time)\n",
    "\n",
    "        print(f\"Run {i + 1}/{n} - Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Calculate the average time and DPS\n",
    "    avg_time = np.mean(times)\n",
    "    dps = ndocs / avg_time\n",
    "\n",
    "    # Calculate standard errors if n > 1\n",
    "    if n > 1:\n",
    "        time_se = np.std(times, ddof=1) / np.sqrt(n)\n",
    "        dps_se = (np.std([ndocs / t for t in times], ddof=1) / np.sqrt(n))\n",
    "    else:\n",
    "        time_se = None\n",
    "        dps_se = None\n",
    "\n",
    "    print(f\"Average elapsed time: {avg_time:.2f} seconds\")\n",
    "    print(f\"Average DPS: {dps}\")\n",
    "\n",
    "    if n > 1:\n",
    "        print(f\"Standard error (Time): {time_se:.4f} seconds\")\n",
    "        print(f\"Standard error (DPS): {dps_se:.4f}\")\n",
    "\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "    res = {\n",
    "        'Model': model.split('/')[-1],\n",
    "        'Hardware': pipe.device.type,\n",
    "        'Time': avg_time,\n",
    "        'Time_SE': time_se,\n",
    "        'DPS': dps,\n",
    "        'DPS_SE': dps_se\n",
    "    }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3 Max Base DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "model = \"mlburnham/Political_DEBATE_DeBERTa_base_v1.1\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = torch.device(\"mps\"), batch_size = 8, torch_dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/1 - Elapsed time: 32.74 seconds\n",
      "Average elapsed time: 32.74 seconds\n",
      "Average DPS: 152.71852848907307\n",
      "Run 1/10 - Elapsed time: 32.31 seconds\n",
      "Run 2/10 - Elapsed time: 32.26 seconds\n",
      "Run 3/10 - Elapsed time: 32.44 seconds\n",
      "Run 4/10 - Elapsed time: 32.51 seconds\n",
      "Run 5/10 - Elapsed time: 32.26 seconds\n",
      "Run 6/10 - Elapsed time: 32.58 seconds\n",
      "Run 7/10 - Elapsed time: 32.84 seconds\n",
      "Run 8/10 - Elapsed time: 32.26 seconds\n",
      "Run 9/10 - Elapsed time: 32.17 seconds\n",
      "Run 10/10 - Elapsed time: 32.61 seconds\n",
      "Average elapsed time: 32.42 seconds\n",
      "Average DPS: 154.20780747542545\n",
      "Standard error (Time): 0.0658 seconds\n",
      "Standard error (DPS): 0.3118\n"
     ]
    }
   ],
   "source": [
    "# once to compile\n",
    "time_it(pipe, n = 1)\n",
    "# benchmark\n",
    "res = time_it(pipe, n = 10)\n",
    "timings.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3 Max Large DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "model = \"mlburnham/Political_DEBATE_large_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = torch.device(\"mps\"), batch_size = 8, torch_dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/1 - Elapsed time: 176.31 seconds\n",
      "Average elapsed time: 176.31 seconds\n",
      "Average DPS: 28.359860299473564\n",
      "Run 1/10 - Elapsed time: 81.72 seconds\n",
      "Run 2/10 - Elapsed time: 82.07 seconds\n",
      "Run 3/10 - Elapsed time: 83.57 seconds\n",
      "Run 4/10 - Elapsed time: 82.88 seconds\n",
      "Run 5/10 - Elapsed time: 82.90 seconds\n",
      "Run 6/10 - Elapsed time: 83.50 seconds\n",
      "Run 7/10 - Elapsed time: 83.54 seconds\n",
      "Run 8/10 - Elapsed time: 82.11 seconds\n",
      "Run 9/10 - Elapsed time: 82.02 seconds\n",
      "Run 10/10 - Elapsed time: 82.37 seconds\n",
      "Average elapsed time: 82.67 seconds\n",
      "Average DPS: 60.48218526077912\n",
      "Standard error (Time): 0.2220 seconds\n",
      "Standard error (DPS): 0.1622\n"
     ]
    }
   ],
   "source": [
    "# once to compile\n",
    "time_it(pipe, n = 1)\n",
    "# benchmark\n",
    "res = time_it(pipe, n = 10)\n",
    "timings.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3 Max Base ModernBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "model = \"mlburnham/Political_DEBATE_ModernBERT_base_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = torch.device(\"mps\"), batch_size = 8, torch_dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model with `torch.compile` and using a `torch.mps` device is not supported. Falling back to non-compiled mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/1 - Elapsed time: 170.65 seconds\n",
      "Average elapsed time: 170.65 seconds\n",
      "Average DPS: 29.299827449098625\n",
      "Run 1/10 - Elapsed time: 41.60 seconds\n",
      "Run 2/10 - Elapsed time: 42.50 seconds\n",
      "Run 3/10 - Elapsed time: 42.04 seconds\n",
      "Run 4/10 - Elapsed time: 42.42 seconds\n",
      "Run 5/10 - Elapsed time: 42.71 seconds\n",
      "Run 6/10 - Elapsed time: 42.37 seconds\n",
      "Run 7/10 - Elapsed time: 42.45 seconds\n",
      "Run 8/10 - Elapsed time: 42.35 seconds\n",
      "Run 9/10 - Elapsed time: 42.32 seconds\n",
      "Run 10/10 - Elapsed time: 41.62 seconds\n",
      "Average elapsed time: 42.24 seconds\n",
      "Average DPS: 118.37369231929557\n",
      "Standard error (Time): 0.1170 seconds\n",
      "Standard error (DPS): 0.3303\n"
     ]
    }
   ],
   "source": [
    "# once to compile\n",
    "time_it(pipe, n = 1)\n",
    "# benchmark\n",
    "res = time_it(pipe, n = 10)\n",
    "timings.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3 Max Large ModernBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "model = \"mlburnham/Political_DEBATE_ModernBERT_large_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = torch.device(\"mps\"), batch_size = 8, torch_dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/1 - Elapsed time: 82.01 seconds\n",
      "Average elapsed time: 82.01 seconds\n",
      "Average DPS: 60.968216981768585\n",
      "Run 1/10 - Elapsed time: 82.49 seconds\n",
      "Run 2/10 - Elapsed time: 83.51 seconds\n",
      "Run 3/10 - Elapsed time: 85.91 seconds\n",
      "Run 4/10 - Elapsed time: 86.97 seconds\n",
      "Run 5/10 - Elapsed time: 86.82 seconds\n",
      "Run 6/10 - Elapsed time: 86.67 seconds\n",
      "Run 7/10 - Elapsed time: 86.37 seconds\n",
      "Run 8/10 - Elapsed time: 86.32 seconds\n",
      "Run 9/10 - Elapsed time: 86.33 seconds\n",
      "Run 10/10 - Elapsed time: 86.49 seconds\n",
      "Average elapsed time: 85.79 seconds\n",
      "Average DPS: 58.28415983234997\n",
      "Standard error (Time): 0.4798 seconds\n",
      "Standard error (DPS): 0.3342\n"
     ]
    }
   ],
   "source": [
    "# once to compile\n",
    "time_it(pipe, n = 1)\n",
    "# benchmark\n",
    "res = time_it(pipe, n = 10)\n",
    "timings.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3 Max Base ModernBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_base_v1.0\"\n",
    "pipe = pipeline(\"zero-shot-classification\", model = model, device = torch.device(\"mps\"), batch_size = 32, torch_dtype = torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/1 - Elapsed time: 32.74 seconds\n",
      "Average elapsed time: 32.74 seconds\n",
      "Average DPS: 152.71852848907307\n",
      "Run 1/10 - Elapsed time: 32.31 seconds\n",
      "Run 2/10 - Elapsed time: 32.26 seconds\n",
      "Run 3/10 - Elapsed time: 32.44 seconds\n",
      "Run 4/10 - Elapsed time: 32.51 seconds\n",
      "Run 5/10 - Elapsed time: 32.26 seconds\n",
      "Run 6/10 - Elapsed time: 32.58 seconds\n",
      "Run 7/10 - Elapsed time: 32.84 seconds\n",
      "Run 8/10 - Elapsed time: 32.26 seconds\n",
      "Run 9/10 - Elapsed time: 32.17 seconds\n",
      "Run 10/10 - Elapsed time: 32.61 seconds\n",
      "Average elapsed time: 32.42 seconds\n",
      "Average DPS: 154.20780747542545\n",
      "Standard error (Time): 0.0658 seconds\n",
      "Standard error (DPS): 0.3118\n"
     ]
    }
   ],
   "source": [
    "# once to compile\n",
    "time_it(pipe, n = 1)\n",
    "# benchmark\n",
    "res = time_it(pipe, n = 10)\n",
    "timings.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22c7ddea1b842c5ac8b5ebb33f83529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "pipe = pipeline(\"text-generation\", model=model, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map='mps', batch_size = 1,\n",
    "token = \"########\")\n",
    "pipe.tokenizer.pad_token_id = pipe.model.config.eos_token_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"You are a classifier that can only respond with 1 or 0. I'm going to show you a short text sample and I want you to determine if this text is about politics. Here is the text:\n",
    "{doc}\n",
    "\n",
    "If it is true that this text is about politics, return 1. If it is not true that this text is about politics, return 0.\n",
    "Do not explain your answer, and only return 1 or 0.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": user_message.format(doc = doc)} for doc in test['premise']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [pipe.tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True) for message in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2219.90 seconds\n",
      "DPS: 2.252349324300326\n"
     ]
    }
   ],
   "source": [
    "torch.mps.empty_cache()\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "results = pipe(prompt, max_new_tokens=2, do_sample=False, return_full_text = False, pad_token_id=pipe.tokenizer.pad_token_id, temperature = 0)\n",
    "# Stop timer\n",
    "end_time = time.time()\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"DPS: {ndocs/elapsed_time}\")\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "timings.append({\n",
    "                'Model': model.split('/')[-1],\n",
    "                'Hardware': 'mps',\n",
    "                'Time': elapsed_time,\n",
    "                'DPS': ndocs/elapsed_time\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps = pd.DataFrame(timings)\n",
    "mps.to_csv('../data/mps_timing.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
