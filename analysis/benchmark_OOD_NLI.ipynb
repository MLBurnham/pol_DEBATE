{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee8f6fb-ee37-4e75-bcce-79ea6a0a7d22",
   "metadata": {},
   "source": [
    "To do:\n",
    "- benchmark llama\n",
    "- benchmark Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fd2c6cff-810a-4922-a1af-51abc8b5f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "778aeaff-3295-4e69-9680-4aabaac5fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_labels(df, label_col = 'label'):\n",
    "    # Recode to binary: 1 for entailment, 0 for neutral/contradiction\n",
    "    example[label_col] = 1 if example[label_col] == 0 else 0\n",
    "    return df\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'MCC': matthews_corrcoef(y_true, y_pred),\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'F1': f1_score(y_true, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "def bootstrapped_errors(results_df, n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Calculate bootstrapped standard errors for MCC, Accuracy, and F1.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pd.DataFrame): DataFrame containing 'entailment' (true labels) and 'label' (predictions).\n",
    "        n_bootstrap (int): Number of bootstrap samples to generate.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Standard errors for MCC, Accuracy, and F1.\n",
    "    \"\"\"\n",
    "    mcc_scores = []\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    y_true = results_df['entailment']\n",
    "    y_pred = results_df['label']\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        y_true_resampled, y_pred_resampled = resample(y_true, y_pred)\n",
    "        \n",
    "        # Calculate metrics for the resampled data\n",
    "        mcc_scores.append(matthews_corrcoef(y_true_resampled, y_pred_resampled))\n",
    "        accuracy_scores.append(accuracy_score(y_true_resampled, y_pred_resampled))\n",
    "        f1_scores.append(f1_score(y_true_resampled, y_pred_resampled, average='weighted'))\n",
    "    \n",
    "    # Calculate standard errors\n",
    "    return {\n",
    "        'MCC_SE': np.std(mcc_scores),\n",
    "        'Accuracy_SE': np.std(accuracy_scores),\n",
    "        'F1_SE': np.std(f1_scores)\n",
    "    }\n",
    "\n",
    "def benchmark(model, data, device = 'mps', textcol = 'premise', hypcol = 'hypothesis', labelcol = 'label'):\n",
    "    \n",
    "    pipe = pipeline(\"zero-shot-classification\", model = model, device = device)\n",
    "    \n",
    "    # Initialize results storage\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    scores = []\n",
    "    \n",
    "    # Process one example at a time\n",
    "    for i in range(len(data)):\n",
    "        premise = data[i][textcol]\n",
    "        hypothesis = data[i][hypcol]\n",
    "        true_label = data[i][labelcol]\n",
    "        \n",
    "        # Run classifier\n",
    "        result = pipe(\n",
    "            premise,\n",
    "            candidate_labels=[hypothesis],\n",
    "            multi_label=False\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        prediction = result['scores'][0]  # Get confidence score\n",
    "        predictions.append(0 if prediction >= 0.5 else 1)  # Convert score to binary prediction\n",
    "        actual_labels.append(true_label)\n",
    "        scores.append(prediction)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results_df = pd.DataFrame({\n",
    "        'label': predictions,\n",
    "        'entailment': actual_labels,\n",
    "        'score': scores\n",
    "    })\n",
    "    \n",
    "    return results_df, get_metrics(results_df['entailment'], results_df['label'])\n",
    "\n",
    "def benchmark_datasets(dataset_dict, model, device):\n",
    "    results = []\n",
    "    \n",
    "    # Add progress bar for datasets\n",
    "    for dataset_name, dataset in tqdm(dataset_dict.items(), desc=\"Datasets Complete\"):\n",
    "        # Recode labels\n",
    "        #dataset = dataset.map(recode_labels, fn_kwargs={'label_col': 'labels'})\n",
    "        \n",
    "        # Benchmark the dataset\n",
    "        results_df, metrics = benchmark(model, dataset, device, textcol='text', labelcol='labels')\n",
    "        \n",
    "        # Calculate bootstrapped standard errors\n",
    "        errors = bootstrapped_errors(results_df)\n",
    "        \n",
    "        # Save results\n",
    "        results.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Observations': len(dataset),\n",
    "            'MCC': metrics['MCC'],\n",
    "            'Accuracy': metrics['Accuracy'],\n",
    "            'F1': metrics['F1'],\n",
    "            'MCC_SE': errors['MCC_SE'],\n",
    "            'Accuracy_SE': errors['Accuracy_SE'],\n",
    "            'F1_SE': errors['F1_SE'],\n",
    "            'Model': model\n",
    "        })\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72a268-85ed-4064-a7ea-af0bf7e99137",
   "metadata": {},
   "source": [
    "# OOD validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cf2a42a6-363b-4680-a708-298fd45af9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ood = pd.read_csv('../data/out_domain_bench.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f54cc-acc3-4620-876b-9013fa3f9442",
   "metadata": {},
   "source": [
    "## DEBATE Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625aa262-3aa5-40c5-bec3-34825b68eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_base_v1.0\"\n",
    "res, metrics = benchmark(model = model, data = Dataset.from_pandas(ood), textcol = 'text', hypcol = 'hypothesis', labelcol = 'labels')\n",
    "ood['debate_base'] = res['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b713ff-38cf-47fa-80a6-81f30e3d25db",
   "metadata": {},
   "source": [
    "## DeBERTa Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7eda21bf-8fc0-4789-9c36-1bee067e7e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MCC': 0.8428277147812402,\n",
       " 'Accuracy': 0.927020921324035,\n",
       " 'F1': 0.9265993784470345}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = 'MoritzLaurer/deberta-v3-base-zeroshot-v2.0'\n",
    "res, metrics = benchmark(model = model, data = Dataset.from_pandas(ood), textcol = 'text', hypcol = 'hypothesis', labelcol = 'labels')\n",
    "ood['deberta_base'] = res['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152089b-e8b4-4316-90c1-a5c8e1c064d0",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "91e9a1a8-1e56-4d60-ada6-4393cbeb7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ood\n",
    "\n",
    "results = []\n",
    "for column in ['debate_base', 'deberta_base']:\n",
    "    metrics = get_metrics(df['labels'], df[column])\n",
    "    metrics_se = bootstrapped_errors(pd.DataFrame({\n",
    "        'entailment': df['labels'],\n",
    "        'label': df[column]\n",
    "    }))\n",
    "    combined_results = {**metrics, **metrics_se, 'Model': column}\n",
    "    results.append(combined_results)\n",
    "\n",
    "# Calculate metrics by task\n",
    "tasks = df['task_name'].unique()\n",
    "for task in tasks:\n",
    "    task_df = df[df['task_name'] == task]\n",
    "    for column in ['debate_base', 'deberta_base']:\n",
    "        metrics = get_metrics(task_df['labels'], task_df[column])\n",
    "        metrics_se = bootstrapped_errors(pd.DataFrame({\n",
    "            'entailment': task_df['labels'],\n",
    "            'label': task_df[column]\n",
    "        }))\n",
    "        combined_results = {**metrics, **metrics_se, 'Model': column, 'Task': task}\n",
    "        results.append(combined_results)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.loc[results_df['Task'].isna(), 'Task'] = 'Overall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a2b99532-c078-4d23-ad31-a918a8147b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllll}\n",
      "\\toprule\n",
      "Model & Overall & agnews & amazonpolarity & emotiondair & go_emotions & rottentomatoes & sst2 & tweet_topic & yahootopics \\\\\n",
      "\\midrule\n",
      "debate_base & $\\mathbf{0.916} \\pm 0.003$ & $\\mathbf{0.963} \\pm 0.005$ & $\\mathbf{0.939} \\pm 0.006$ & $\\mathbf{0.942} \\pm 0.005$ & $\\mathbf{0.846} \\pm 0.011$ & $\\mathbf{0.874} \\pm 0.009$ & $\\mathbf{0.903} \\pm 0.008$ & $\\mathbf{0.964} \\pm 0.004$ & $\\mathbf{0.935} \\pm 0.006$ \\\\\n",
      "deberta_base & $\\mathbf{0.927} \\pm 0.003$ & $\\mathbf{0.966} \\pm 0.005$ & $\\mathbf{0.974} \\pm 0.004$ & $\\mathbf{0.966} \\pm 0.004$ & $\\mathbf{0.887} \\pm 0.010$ & $\\mathbf{0.951} \\pm 0.006$ & $\\mathbf{0.965} \\pm 0.005$ & $\\mathbf{0.819} \\pm 0.010$ & $\\mathbf{0.964} \\pm 0.005$ \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pivot table for F1 and F1_SE\n",
    "f1_table = results_df.pivot(index='Model', columns='Task', values=['F1', 'F1_SE'])\n",
    "\n",
    "# Identify the largest F1 scores per task\n",
    "max_f1_per_task = f1_table['F1'].idxmax()\n",
    "\n",
    "# Format the F1 scores with standard errors and highlight the max values\n",
    "def format_f1(row, max_index):\n",
    "    return row['F1'].combine(row['F1_SE'], \n",
    "        lambda f1, se, task=row.name: f\"$\\\\mathbf{{{f1:.3f}}} \\\\pm {se:.3f}$\" if task in max_index.values else f\"${f1:.3f} \\\\pm {se:.3f}$\")\n",
    "\n",
    "formatted_f1_table = f1_table.apply(\n",
    "    lambda row: format_f1(row, max_f1_per_task), axis=1\n",
    ")\n",
    "\n",
    "# Reset index for better formatting\n",
    "formatted_f1_table = formatted_f1_table.reset_index()\n",
    "\n",
    "# Convert to LaTeX\n",
    "latex_table = formatted_f1_table.to_latex(index=False, escape=False, multicolumn=True, float_format=\"%.3f\")\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a4e92-9088-4ec2-a50c-1f8a00e6e951",
   "metadata": {},
   "source": [
    "# NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ac8545ae-7d4f-45b0-a6c7-3125fd58d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nli = pd.read_csv('../data/nli_bench.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "109c770d-1cca-42c9-8d8c-a8bdbc3227bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_base_v1.0\"\n",
    "res, metrics = benchmark(model = model, data = Dataset.from_pandas(nli), textcol = 'text', hypcol = 'hypothesis', labelcol = 'labels')\n",
    "nli['debate_base'] = res['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "56efe583-4357-4796-b1c6-2d7fecc78a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'MoritzLaurer/deberta-v3-base-zeroshot-v2.0'\n",
    "res, metrics = benchmark(model = model, data = Dataset.from_pandas(nli), textcol = 'text', hypcol = 'hypothesis', labelcol = 'labels')\n",
    "nli['deberta_base'] = res['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e679ad-e7cd-42ff-8716-e963c193bcd9",
   "metadata": {},
   "source": [
    "## Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "d601293b-9479-4bde-8511-941c3c0e4f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = nli\n",
    "\n",
    "results = []\n",
    "for column in ['debate_base', 'deberta_base']:\n",
    "    metrics = get_metrics(df['labels'], df[column])\n",
    "    metrics_se = bootstrapped_errors(pd.DataFrame({\n",
    "        'entailment': df['labels'],\n",
    "        'label': df[column]\n",
    "    }))\n",
    "    combined_results = {**metrics, **metrics_se, 'Model': column}\n",
    "    results.append(combined_results)\n",
    "\n",
    "# Calculate metrics by task\n",
    "tasks = df['task_name'].unique()\n",
    "for task in tasks:\n",
    "    task_df = df[df['task_name'] == task]\n",
    "    for column in ['debate_base', 'deberta_base']:\n",
    "        metrics = get_metrics(task_df['labels'], task_df[column])\n",
    "        metrics_se = bootstrapped_errors(pd.DataFrame({\n",
    "            'entailment': task_df['labels'],\n",
    "            'label': task_df[column]\n",
    "        }))\n",
    "        combined_results = {**metrics, **metrics_se, 'Model': column, 'Task': task}\n",
    "        results.append(combined_results)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.loc[results_df['Task'].isna(), 'Task'] = 'Overall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "71c8c906-4bec-4eaf-a679-f779424dedba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "Model & Overall & anli_r1 & mnli_m & wanli \\\\\n",
      "\\midrule\n",
      "debate_base & $0.660 \\pm 0.009$ & $0.556 \\pm 0.016$ & $0.771 \\pm 0.013$ & $0.649 \\pm 0.015$ \\\\\n",
      "deberta_base & $\\mathbf{0.857} \\pm 0.006$ & $\\mathbf{0.822} \\pm 0.012$ & $\\mathbf{0.945} \\pm 0.007$ & $\\mathbf{0.804} \\pm 0.012$ \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pivot table for F1 and F1_SE\n",
    "f1_table = results_df.pivot(index='Model', columns='Task', values=['F1', 'F1_SE'])\n",
    "\n",
    "# Identify the largest F1 scores per task\n",
    "max_f1_per_task = f1_table['F1'].idxmax()\n",
    "\n",
    "# Format the F1 scores with standard errors and highlight the max values\n",
    "def format_f1(row, max_index):\n",
    "    return row['F1'].combine(row['F1_SE'], \n",
    "        lambda f1, se, task=row.name: f\"$\\\\mathbf{{{f1:.3f}}} \\\\pm {se:.3f}$\" if task in max_index.values else f\"${f1:.3f} \\\\pm {se:.3f}$\")\n",
    "\n",
    "formatted_f1_table = f1_table.apply(\n",
    "    lambda row: format_f1(row, max_f1_per_task), axis=1\n",
    ")\n",
    "\n",
    "# Reset index for better formatting\n",
    "formatted_f1_table = formatted_f1_table.reset_index()\n",
    "\n",
    "# Convert to LaTeX\n",
    "latex_table = formatted_f1_table.to_latex(index=False, escape=False, multicolumn=True, float_format=\"%.3f\")\n",
    "\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
