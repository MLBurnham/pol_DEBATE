{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee8f6fb-ee37-4e75-bcce-79ea6a0a7d22",
   "metadata": {},
   "source": [
    "To do:\n",
    "- benchmark llama\n",
    "- benchmark Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2c6cff-810a-4922-a1af-51abc8b5f59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mb7336/miniforge3/envs/sandbox/lib/python3.11/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/mb7336/miniforge3/envs/sandbox/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <EB3FF92A-5EB1-3EE8-AF8B-5923C1265422> /Users/mb7336/miniforge3/envs/sandbox/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/Users/mb7336/miniforge3/envs/sandbox/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/mb7336/miniforge3/envs/sandbox/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/mb7336/miniforge3/envs/sandbox/lib/python3.11/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/mb7336/miniforge3/envs/sandbox/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0059835b-3a11-4e1a-87c9-3718b7cabd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(df, preds, group_by=None):\n",
    "    \"\"\"\n",
    "    Calculate MCC, Accuracy, F1 for predictions.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing true and predicted labels.\n",
    "        preds (list): List of column names containing model predictions.\n",
    "        group_by (str, optional): Column name to group by ('dataset' or 'task'). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with calculated metrics, optionally grouped by `group_by`.\n",
    "    \"\"\"\n",
    "    true_col = 'entailment'\n",
    "    \n",
    "    def get_metrics(y_true, y_pred):\n",
    "        return {\n",
    "            'MCC': matthews_corrcoef(y_true, y_pred),\n",
    "            'Accuracy': accuracy_score(y_true, y_pred),\n",
    "            'F1': f1_score(y_true, y_pred, average='weighted')\n",
    "        }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if group_by not in ['dataset', 'task']:\n",
    "        for col in preds:\n",
    "            metrics = get_metrics(df[true_col], df[col])\n",
    "            metrics['Column'] = col\n",
    "            results.append(metrics)\n",
    "    else:\n",
    "        for col in preds:\n",
    "            for group_name, group in df.groupby(group_by):\n",
    "                metrics = get_metrics(group[true_col], group[col])\n",
    "                metrics['Column'] = col\n",
    "                metrics[group_by.capitalize()] = group_name\n",
    "                results.append(metrics)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if group_by in ['dataset', 'task']:\n",
    "        return results_df.set_index(['Column', group_by.capitalize()])\n",
    "    else:\n",
    "        return results_df.set_index('Column')\n",
    "\n",
    "def bootstrapped_errors(y_true, y_pred, n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Calculate bootstrapped standard errors for MCC, Accuracy, and F1.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        n_bootstrap (int, optional): Number of bootstrap samples. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        dict: Standard errors for MCC, Accuracy, and F1.\n",
    "    \"\"\"\n",
    "    mcc_scores = []\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        y_true_resampled, y_pred_resampled = resample(y_true, y_pred)\n",
    "        \n",
    "        # Calculate metrics for the resampled data\n",
    "        mcc_scores.append(matthews_corrcoef(y_true_resampled, y_pred_resampled))\n",
    "        accuracy_scores.append(accuracy_score(y_true_resampled, y_pred_resampled))\n",
    "        f1_scores.append(f1_score(y_true_resampled, y_pred_resampled, average='weighted'))\n",
    "    \n",
    "    # Calculate standard errors\n",
    "    return {\n",
    "        'MCC_SE': np.std(mcc_scores),\n",
    "        'Accuracy_SE': np.std(accuracy_scores),\n",
    "        'F1_SE': np.std(f1_scores)\n",
    "    }\n",
    "\n",
    "def metrics_with_errors(df, preds, n_bootstrap=1000, group_by=None):\n",
    "    \"\"\"\n",
    "    Calculate metrics and bootstrapped standard errors for predictions, optionally grouped.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing true and predicted labels.\n",
    "        preds (list): List of column names containing model predictions.\n",
    "        n_bootstrap (int, optional): Number of bootstrap samples. Defaults to 1000.\n",
    "        group_by (str, optional): Column name to group by ('dataset' or 'task'). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of metrics, standard errors, and confidence intervals.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate metrics for each model\n",
    "    metrics_df = metrics(df, preds, group_by=group_by)\n",
    "\n",
    "    # Step 2: Calculate bootstrapped errors for each model or group\n",
    "    errors = []\n",
    "    if group_by not in ['dataset', 'task']:\n",
    "        for col in preds:\n",
    "            y_true = df['entailment']\n",
    "            y_pred = df[col]\n",
    "            errors_dict = bootstrapped_errors(y_true, y_pred, n_bootstrap=n_bootstrap)\n",
    "            errors_dict['Column'] = col\n",
    "            errors.append(errors_dict)\n",
    "    else:\n",
    "        for col in preds:\n",
    "            for group_name, group in df.groupby(group_by):\n",
    "                y_true = group['entailment']\n",
    "                y_pred = group[col]\n",
    "                errors_dict = bootstrapped_errors(y_true, y_pred, n_bootstrap=n_bootstrap)\n",
    "                errors_dict['Column'] = col\n",
    "                errors_dict[group_by.capitalize()] = group_name\n",
    "                errors.append(errors_dict)\n",
    "\n",
    "    errors_df = pd.DataFrame(errors)\n",
    "\n",
    "    if group_by in ['dataset', 'task']:\n",
    "        errors_df = errors_df.set_index(['Column', group_by.capitalize()])\n",
    "    else:\n",
    "        errors_df = errors_df.set_index('Column')\n",
    "\n",
    "    # Step 3: Merge metrics and errors DataFrames\n",
    "    combined_df = metrics_df.merge(errors_df, left_index=True, right_index=True)\n",
    "\n",
    "    # Step 4: Calculate confidence intervals (upper and lower bounds)\n",
    "    combined_df['MCC_Lower'] = combined_df['MCC'] - combined_df['MCC_SE']\n",
    "    combined_df['MCC_Upper'] = combined_df['MCC'] + combined_df['MCC_SE']\n",
    "\n",
    "    combined_df['Accuracy_Lower'] = combined_df['Accuracy'] - combined_df['Accuracy_SE']\n",
    "    combined_df['Accuracy_Upper'] = combined_df['Accuracy'] + combined_df['Accuracy_SE']\n",
    "\n",
    "    combined_df['F1_Lower'] = combined_df['F1'] - combined_df['F1_SE']\n",
    "    combined_df['F1_Upper'] = combined_df['F1'] + combined_df['F1_SE']\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "def label_docs(model, docs_dict, batch_size = 8, device = 'cuda'):\n",
    "    \"\"\"\n",
    "    Passes documents through the pipeline. Returns a list of entail, not_entail labels\n",
    "    \"\"\"\n",
    "    pipe = pipeline(task = 'text-classification', model = model, \n",
    "                    batch_size = batch_size, device = device, \n",
    "                    max_length = 512, truncation = True, \n",
    "                    torch_dtype = torch.bfloat16)\n",
    "    res = pipe(docs_dict)\n",
    "    res = [result['label'] for result in res]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "778aeaff-3295-4e69-9680-4aabaac5fc33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def recode_labels(df, label_col = 'label'):\n",
    "    # Recode to binary: 1 for entailment, 0 for neutral/contradiction\n",
    "    example[label_col] = 1 if example[label_col] == 0 else 0\n",
    "    return df\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'MCC': matthews_corrcoef(y_true, y_pred),\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'F1': f1_score(y_true, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "def bootstrapped_errors(y_true, y_pred, n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Calculate bootstrapped standard errors for MCC, Accuracy, and F1.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pd.DataFrame): DataFrame containing 'entailment' (true labels) and 'label' (predictions).\n",
    "        n_bootstrap (int): Number of bootstrap samples to generate.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Standard errors for MCC, Accuracy, and F1.\n",
    "    \"\"\"\n",
    "    mcc_scores = []\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        y_true_resampled, y_pred_resampled = resample(y_true, y_pred)\n",
    "        \n",
    "        # Calculate metrics for the resampled data\n",
    "        mcc_scores.append(matthews_corrcoef(y_true_resampled, y_pred_resampled))\n",
    "        accuracy_scores.append(accuracy_score(y_true_resampled, y_pred_resampled))\n",
    "        f1_scores.append(f1_score(y_true_resampled, y_pred_resampled, average='weighted'))\n",
    "    \n",
    "    # Calculate standard errors\n",
    "    return {\n",
    "        'MCC_SE': np.std(mcc_scores),\n",
    "        'Accuracy_SE': np.std(accuracy_scores),\n",
    "        'F1_SE': np.std(f1_scores)\n",
    "    }\n",
    "\n",
    "def label_docs(model, docs_dict, batch_size = 32, device = 'cuda'):\n",
    "    \"\"\"\n",
    "    Passes documents through the pipeline. Returns a list of entail, not_entail labels\n",
    "    \"\"\"\n",
    "    pipe = pipeline(task = 'text-classification', model = model, \n",
    "                    batch_size = batch_size, device = device, \n",
    "                    max_length = 512, truncation = True, \n",
    "                    torch_dtype = torch.float16)\n",
    "    res = pipe(docs_dict)\n",
    "    res = [result['label'] for result in res]\n",
    "    return res\n",
    "\n",
    "def benchmark(model, data, device = 'mps', textcol = 'premise', hypcol = 'hypothesis', labelcol = 'label'):\n",
    "    \n",
    "    pipe = pipeline(\"zero-shot-classification\", model = model, device = device)\n",
    "    \n",
    "    # Initialize results storage\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    scores = []\n",
    "    \n",
    "    # Process one example at a time\n",
    "    for i in range(len(data)):\n",
    "        premise = data[i][textcol]\n",
    "        hypothesis = data[i][hypcol]\n",
    "        true_label = data[i][labelcol]\n",
    "        \n",
    "        # Run classifier\n",
    "        result = pipe(\n",
    "            premise,\n",
    "            candidate_labels=[hypothesis],\n",
    "            multi_label=False\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        prediction = result['scores'][0]  # Get confidence score\n",
    "        predictions.append(0 if prediction >= 0.5 else 1)  # Convert score to binary prediction\n",
    "        actual_labels.append(true_label)\n",
    "        scores.append(prediction)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results_df = pd.DataFrame({\n",
    "        'label': predictions,\n",
    "        'entailment': actual_labels,\n",
    "        'score': scores\n",
    "    })\n",
    "    \n",
    "    return results_df, get_metrics(results_df['entailment'], results_df['label'])\n",
    "\n",
    "def benchmark_datasets(dataset_dict, model, device):\n",
    "    results = []\n",
    "    \n",
    "    # Add progress bar for datasets\n",
    "    for dataset_name, dataset in tqdm(dataset_dict.items(), desc=\"Datasets Complete\"):\n",
    "        # Recode labels\n",
    "        #dataset = dataset.map(recode_labels, fn_kwargs={'label_col': 'labels'})\n",
    "        \n",
    "        # Benchmark the dataset\n",
    "        results_df, metrics = benchmark(model, dataset, device, textcol='text', labelcol='labels')\n",
    "        \n",
    "        # Calculate bootstrapped standard errors\n",
    "        errors = bootstrapped_errors(results_df)\n",
    "        \n",
    "        # Save results\n",
    "        results.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Observations': len(dataset),\n",
    "            'MCC': metrics['MCC'],\n",
    "            'Accuracy': metrics['Accuracy'],\n",
    "            'F1': metrics['F1'],\n",
    "            'MCC_SE': errors['MCC_SE'],\n",
    "            'Accuracy_SE': errors['Accuracy_SE'],\n",
    "            'F1_SE': errors['F1_SE'],\n",
    "            'Model': model\n",
    "        })\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72a268-85ed-4064-a7ea-af0bf7e99137",
   "metadata": {},
   "source": [
    "# OOD validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8909ac35-a764-4336-a8d3-7c238a5ef031",
   "metadata": {},
   "outputs": [],
   "source": [
    "ood = pd.read_csv('../data/out_domain_bench.csv')\n",
    "ood.rename({'task_name':'task', 'labels':'entailment'}, axis = 1, inplace = True)\n",
    "docs_dict = [{'text':ood.loc[i, 'text'], 'text_pair':ood.loc[i, 'hypothesis']} for i in ood.index]\n",
    "\n",
    "# models that will be tested\n",
    "models = [\"MoritzLaurer/deberta-v3-base-zeroshot-v2.0\", \n",
    "          \"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\",\n",
    "          \"mlburnham/Political_DEBATE_DeBERTa_base_v1.1\",\n",
    "          \"mlburnham/Political_DEBATE_large_v1.0\",\n",
    "          \"mlburnham/Political_DEBATE_ModernBERT_base_v1.0\",\n",
    "          \"mlburnham/Political_DEBATE_ModernBERT_large_v1.0\"]\n",
    "\n",
    "# column names that will hold results\n",
    "columns = ['base_nli',\n",
    "           'large_nli',\n",
    "           'base_debate',\n",
    "           'large_debate',\n",
    "           'base_modern',\n",
    "           'large_modern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06fff7c6-b2e0-4594-a7b2-1571e5579a8d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "<timed exec>:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoritzLaurer/deberta-v3-base-zeroshot-v2.0 complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "<timed exec>:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoritzLaurer/deberta-v3-large-zeroshot-v2.0 complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "<timed exec>:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlburnham/Political_DEBATE_DeBERTa_base_v1.1 complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "<timed exec>:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlburnham/Political_DEBATE_large_v1.0 complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "Compiling the model with `torch.compile` and using a `torch.mps` device is not supported. Falling back to non-compiled mode.\n",
      "<timed exec>:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlburnham/Political_DEBATE_ModernBERT_base_v1.0 complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlburnham/Political_DEBATE_ModernBERT_large_v1.0 complete.\n",
      "CPU times: user 9min 43s, sys: 1min 26s, total: 11min 10s\n",
      "Wall time: 15min 30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for each model, classify documents and return labels to the test dataframe\n",
    "for modname, col in zip(models, columns):\n",
    "    res = label_docs(modname, docs_dict, batch_size = 8, device = 'mps')\n",
    "    ood[col] = res\n",
    "    ood[col] = ood[col].replace({'entailment': 0, 'not_entailment': 1})\n",
    "    print(modname + ' complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "671a9696-031c-42f4-be76-f0d52da9ed60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 884 ms, total: 1min 32s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate performance metrics with bootstrapped standard errors. n_bootstrap == 1000\n",
    "overall = metrics_with_errors(ood, columns, group_by = None)\n",
    "task = metrics_with_errors(ood, columns, group_by = 'task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e82c103e-1286-4de2-9fea-7be63b53751e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>MCC_SE</th>\n",
       "      <th>Accuracy_SE</th>\n",
       "      <th>F1_SE</th>\n",
       "      <th>MCC_Lower</th>\n",
       "      <th>MCC_Upper</th>\n",
       "      <th>Accuracy_Lower</th>\n",
       "      <th>Accuracy_Upper</th>\n",
       "      <th>F1_Lower</th>\n",
       "      <th>F1_Upper</th>\n",
       "      <th>Task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base_nli</td>\n",
       "      <td>0.837733</td>\n",
       "      <td>0.924664</td>\n",
       "      <td>0.924041</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>0.832046</td>\n",
       "      <td>0.843419</td>\n",
       "      <td>0.921997</td>\n",
       "      <td>0.927330</td>\n",
       "      <td>0.921334</td>\n",
       "      <td>0.926749</td>\n",
       "      <td>overall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>large_nli</td>\n",
       "      <td>0.815963</td>\n",
       "      <td>0.914448</td>\n",
       "      <td>0.913286</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.809997</td>\n",
       "      <td>0.821929</td>\n",
       "      <td>0.911642</td>\n",
       "      <td>0.917255</td>\n",
       "      <td>0.910408</td>\n",
       "      <td>0.916164</td>\n",
       "      <td>overall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>base_debate</td>\n",
       "      <td>0.812598</td>\n",
       "      <td>0.912680</td>\n",
       "      <td>0.912558</td>\n",
       "      <td>0.005875</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>0.806722</td>\n",
       "      <td>0.818473</td>\n",
       "      <td>0.909942</td>\n",
       "      <td>0.915419</td>\n",
       "      <td>0.909809</td>\n",
       "      <td>0.915307</td>\n",
       "      <td>overall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>large_debate</td>\n",
       "      <td>0.845425</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.927611</td>\n",
       "      <td>0.005562</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.839863</td>\n",
       "      <td>0.850986</td>\n",
       "      <td>0.925590</td>\n",
       "      <td>0.930809</td>\n",
       "      <td>0.924961</td>\n",
       "      <td>0.930262</td>\n",
       "      <td>overall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>base_modern</td>\n",
       "      <td>0.777288</td>\n",
       "      <td>0.896670</td>\n",
       "      <td>0.896180</td>\n",
       "      <td>0.006534</td>\n",
       "      <td>0.003025</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.770755</td>\n",
       "      <td>0.783822</td>\n",
       "      <td>0.893645</td>\n",
       "      <td>0.899696</td>\n",
       "      <td>0.893125</td>\n",
       "      <td>0.899235</td>\n",
       "      <td>overall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>large_modern</td>\n",
       "      <td>0.827164</td>\n",
       "      <td>0.919556</td>\n",
       "      <td>0.919379</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.821197</td>\n",
       "      <td>0.833130</td>\n",
       "      <td>0.916784</td>\n",
       "      <td>0.922328</td>\n",
       "      <td>0.916597</td>\n",
       "      <td>0.922160</td>\n",
       "      <td>overall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base_nli</td>\n",
       "      <td>0.900427</td>\n",
       "      <td>0.964126</td>\n",
       "      <td>0.964741</td>\n",
       "      <td>0.014383</td>\n",
       "      <td>0.005234</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.886045</td>\n",
       "      <td>0.914810</td>\n",
       "      <td>0.958892</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.959682</td>\n",
       "      <td>0.969800</td>\n",
       "      <td>agnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>base_nli</td>\n",
       "      <td>0.947675</td>\n",
       "      <td>0.973852</td>\n",
       "      <td>0.973859</td>\n",
       "      <td>0.008174</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.939501</td>\n",
       "      <td>0.955849</td>\n",
       "      <td>0.969758</td>\n",
       "      <td>0.977946</td>\n",
       "      <td>0.969768</td>\n",
       "      <td>0.977951</td>\n",
       "      <td>amazonpolarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>base_nli</td>\n",
       "      <td>0.735740</td>\n",
       "      <td>0.960876</td>\n",
       "      <td>0.965640</td>\n",
       "      <td>0.032763</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>0.702977</td>\n",
       "      <td>0.768502</td>\n",
       "      <td>0.955443</td>\n",
       "      <td>0.966310</td>\n",
       "      <td>0.961330</td>\n",
       "      <td>0.969950</td>\n",
       "      <td>emotiondair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>base_nli</td>\n",
       "      <td>0.766265</td>\n",
       "      <td>0.894636</td>\n",
       "      <td>0.894847</td>\n",
       "      <td>0.020706</td>\n",
       "      <td>0.009403</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.745559</td>\n",
       "      <td>0.786971</td>\n",
       "      <td>0.885233</td>\n",
       "      <td>0.904039</td>\n",
       "      <td>0.885488</td>\n",
       "      <td>0.904206</td>\n",
       "      <td>go_emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>base_nli</td>\n",
       "      <td>0.910391</td>\n",
       "      <td>0.955404</td>\n",
       "      <td>0.955448</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.005522</td>\n",
       "      <td>0.005511</td>\n",
       "      <td>0.899300</td>\n",
       "      <td>0.921482</td>\n",
       "      <td>0.949883</td>\n",
       "      <td>0.960926</td>\n",
       "      <td>0.949938</td>\n",
       "      <td>0.960959</td>\n",
       "      <td>rottentomatoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>base_nli</td>\n",
       "      <td>0.933354</td>\n",
       "      <td>0.966936</td>\n",
       "      <td>0.966910</td>\n",
       "      <td>0.009725</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>0.923628</td>\n",
       "      <td>0.943079</td>\n",
       "      <td>0.962106</td>\n",
       "      <td>0.971766</td>\n",
       "      <td>0.962072</td>\n",
       "      <td>0.971747</td>\n",
       "      <td>sst2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>base_nli</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.657724</td>\n",
       "      <td>0.793527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014220</td>\n",
       "      <td>0.010345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.643504</td>\n",
       "      <td>0.671945</td>\n",
       "      <td>0.783181</td>\n",
       "      <td>0.803872</td>\n",
       "      <td>tweet_topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>base_nli</td>\n",
       "      <td>0.747604</td>\n",
       "      <td>0.964206</td>\n",
       "      <td>0.964206</td>\n",
       "      <td>0.035649</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>0.005019</td>\n",
       "      <td>0.711955</td>\n",
       "      <td>0.783253</td>\n",
       "      <td>0.959227</td>\n",
       "      <td>0.969184</td>\n",
       "      <td>0.959187</td>\n",
       "      <td>0.969225</td>\n",
       "      <td>yahootopics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>large_nli</td>\n",
       "      <td>0.926942</td>\n",
       "      <td>0.973842</td>\n",
       "      <td>0.974220</td>\n",
       "      <td>0.011772</td>\n",
       "      <td>0.004254</td>\n",
       "      <td>0.004137</td>\n",
       "      <td>0.915169</td>\n",
       "      <td>0.938714</td>\n",
       "      <td>0.969587</td>\n",
       "      <td>0.978096</td>\n",
       "      <td>0.970083</td>\n",
       "      <td>0.978357</td>\n",
       "      <td>agnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>large_nli</td>\n",
       "      <td>0.966069</td>\n",
       "      <td>0.983039</td>\n",
       "      <td>0.983043</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.959242</td>\n",
       "      <td>0.972896</td>\n",
       "      <td>0.979623</td>\n",
       "      <td>0.986455</td>\n",
       "      <td>0.979629</td>\n",
       "      <td>0.986457</td>\n",
       "      <td>amazonpolarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>large_nli</td>\n",
       "      <td>0.687366</td>\n",
       "      <td>0.949139</td>\n",
       "      <td>0.956593</td>\n",
       "      <td>0.033173</td>\n",
       "      <td>0.006147</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>0.654193</td>\n",
       "      <td>0.720539</td>\n",
       "      <td>0.942992</td>\n",
       "      <td>0.955287</td>\n",
       "      <td>0.951896</td>\n",
       "      <td>0.961289</td>\n",
       "      <td>emotiondair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>large_nli</td>\n",
       "      <td>0.776389</td>\n",
       "      <td>0.899425</td>\n",
       "      <td>0.899527</td>\n",
       "      <td>0.020746</td>\n",
       "      <td>0.009459</td>\n",
       "      <td>0.009431</td>\n",
       "      <td>0.755643</td>\n",
       "      <td>0.797135</td>\n",
       "      <td>0.889967</td>\n",
       "      <td>0.908884</td>\n",
       "      <td>0.890096</td>\n",
       "      <td>0.908958</td>\n",
       "      <td>go_emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>large_nli</td>\n",
       "      <td>0.919530</td>\n",
       "      <td>0.959940</td>\n",
       "      <td>0.959979</td>\n",
       "      <td>0.010268</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.909262</td>\n",
       "      <td>0.929798</td>\n",
       "      <td>0.954809</td>\n",
       "      <td>0.965070</td>\n",
       "      <td>0.954858</td>\n",
       "      <td>0.965100</td>\n",
       "      <td>rottentomatoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>large_nli</td>\n",
       "      <td>0.931839</td>\n",
       "      <td>0.966201</td>\n",
       "      <td>0.966182</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.005036</td>\n",
       "      <td>0.921718</td>\n",
       "      <td>0.941959</td>\n",
       "      <td>0.961170</td>\n",
       "      <td>0.971233</td>\n",
       "      <td>0.961146</td>\n",
       "      <td>0.971218</td>\n",
       "      <td>sst2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>large_nli</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.529140</td>\n",
       "      <td>0.692075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015944</td>\n",
       "      <td>0.013635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.513196</td>\n",
       "      <td>0.545084</td>\n",
       "      <td>0.678440</td>\n",
       "      <td>0.705710</td>\n",
       "      <td>tweet_topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>large_nli</td>\n",
       "      <td>0.809161</td>\n",
       "      <td>0.974646</td>\n",
       "      <td>0.973384</td>\n",
       "      <td>0.031495</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>0.777666</td>\n",
       "      <td>0.840657</td>\n",
       "      <td>0.970395</td>\n",
       "      <td>0.978896</td>\n",
       "      <td>0.968726</td>\n",
       "      <td>0.978043</td>\n",
       "      <td>yahootopics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>base_debate</td>\n",
       "      <td>0.926206</td>\n",
       "      <td>0.973842</td>\n",
       "      <td>0.974163</td>\n",
       "      <td>0.012312</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>0.913895</td>\n",
       "      <td>0.938518</td>\n",
       "      <td>0.969447</td>\n",
       "      <td>0.978236</td>\n",
       "      <td>0.969878</td>\n",
       "      <td>0.978448</td>\n",
       "      <td>agnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>base_debate</td>\n",
       "      <td>0.895334</td>\n",
       "      <td>0.947703</td>\n",
       "      <td>0.947654</td>\n",
       "      <td>0.011989</td>\n",
       "      <td>0.006002</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.883345</td>\n",
       "      <td>0.907323</td>\n",
       "      <td>0.941701</td>\n",
       "      <td>0.953705</td>\n",
       "      <td>0.941642</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>amazonpolarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>base_debate</td>\n",
       "      <td>0.544559</td>\n",
       "      <td>0.898279</td>\n",
       "      <td>0.920289</td>\n",
       "      <td>0.028668</td>\n",
       "      <td>0.008051</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.515891</td>\n",
       "      <td>0.573227</td>\n",
       "      <td>0.890228</td>\n",
       "      <td>0.906329</td>\n",
       "      <td>0.914632</td>\n",
       "      <td>0.925945</td>\n",
       "      <td>emotiondair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>base_debate</td>\n",
       "      <td>0.682917</td>\n",
       "      <td>0.853448</td>\n",
       "      <td>0.854981</td>\n",
       "      <td>0.023583</td>\n",
       "      <td>0.010923</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>0.659334</td>\n",
       "      <td>0.706500</td>\n",
       "      <td>0.842526</td>\n",
       "      <td>0.864371</td>\n",
       "      <td>0.844254</td>\n",
       "      <td>0.865708</td>\n",
       "      <td>go_emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>base_debate</td>\n",
       "      <td>0.765076</td>\n",
       "      <td>0.883598</td>\n",
       "      <td>0.883112</td>\n",
       "      <td>0.017637</td>\n",
       "      <td>0.008798</td>\n",
       "      <td>0.008867</td>\n",
       "      <td>0.747440</td>\n",
       "      <td>0.782713</td>\n",
       "      <td>0.874800</td>\n",
       "      <td>0.892395</td>\n",
       "      <td>0.874245</td>\n",
       "      <td>0.891979</td>\n",
       "      <td>rottentomatoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>base_debate</td>\n",
       "      <td>0.792594</td>\n",
       "      <td>0.895665</td>\n",
       "      <td>0.894794</td>\n",
       "      <td>0.016095</td>\n",
       "      <td>0.008285</td>\n",
       "      <td>0.008421</td>\n",
       "      <td>0.776499</td>\n",
       "      <td>0.808689</td>\n",
       "      <td>0.887380</td>\n",
       "      <td>0.903950</td>\n",
       "      <td>0.886374</td>\n",
       "      <td>0.903215</td>\n",
       "      <td>sst2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>base_debate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884366</td>\n",
       "      <td>0.938635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009539</td>\n",
       "      <td>0.005377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.874828</td>\n",
       "      <td>0.893905</td>\n",
       "      <td>0.933258</td>\n",
       "      <td>0.944012</td>\n",
       "      <td>tweet_topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>base_debate</td>\n",
       "      <td>0.650251</td>\n",
       "      <td>0.943326</td>\n",
       "      <td>0.946393</td>\n",
       "      <td>0.036021</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>0.005637</td>\n",
       "      <td>0.614230</td>\n",
       "      <td>0.686271</td>\n",
       "      <td>0.937178</td>\n",
       "      <td>0.949474</td>\n",
       "      <td>0.940755</td>\n",
       "      <td>0.952030</td>\n",
       "      <td>yahootopics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>large_debate</td>\n",
       "      <td>0.926567</td>\n",
       "      <td>0.973842</td>\n",
       "      <td>0.974192</td>\n",
       "      <td>0.011797</td>\n",
       "      <td>0.004297</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.914770</td>\n",
       "      <td>0.938364</td>\n",
       "      <td>0.969544</td>\n",
       "      <td>0.978139</td>\n",
       "      <td>0.970010</td>\n",
       "      <td>0.978374</td>\n",
       "      <td>agnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>large_debate</td>\n",
       "      <td>0.954652</td>\n",
       "      <td>0.977385</td>\n",
       "      <td>0.977382</td>\n",
       "      <td>0.008036</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>0.946616</td>\n",
       "      <td>0.962687</td>\n",
       "      <td>0.973371</td>\n",
       "      <td>0.981400</td>\n",
       "      <td>0.973365</td>\n",
       "      <td>0.981398</td>\n",
       "      <td>amazonpolarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>large_debate</td>\n",
       "      <td>0.670214</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.953072</td>\n",
       "      <td>0.030677</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>0.004578</td>\n",
       "      <td>0.639538</td>\n",
       "      <td>0.700891</td>\n",
       "      <td>0.938404</td>\n",
       "      <td>0.950485</td>\n",
       "      <td>0.948494</td>\n",
       "      <td>0.957649</td>\n",
       "      <td>emotiondair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>large_debate</td>\n",
       "      <td>0.731472</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.879392</td>\n",
       "      <td>0.021973</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>0.709499</td>\n",
       "      <td>0.753444</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>0.889218</td>\n",
       "      <td>0.869494</td>\n",
       "      <td>0.889290</td>\n",
       "      <td>go_emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>large_debate</td>\n",
       "      <td>0.861807</td>\n",
       "      <td>0.930461</td>\n",
       "      <td>0.930021</td>\n",
       "      <td>0.014169</td>\n",
       "      <td>0.007290</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.847639</td>\n",
       "      <td>0.875976</td>\n",
       "      <td>0.923171</td>\n",
       "      <td>0.937751</td>\n",
       "      <td>0.922645</td>\n",
       "      <td>0.937398</td>\n",
       "      <td>rottentomatoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>large_debate</td>\n",
       "      <td>0.871802</td>\n",
       "      <td>0.934607</td>\n",
       "      <td>0.934111</td>\n",
       "      <td>0.012575</td>\n",
       "      <td>0.006662</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.859226</td>\n",
       "      <td>0.884377</td>\n",
       "      <td>0.927945</td>\n",
       "      <td>0.941269</td>\n",
       "      <td>0.927349</td>\n",
       "      <td>0.940872</td>\n",
       "      <td>sst2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>large_debate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.779833</td>\n",
       "      <td>0.876299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012944</td>\n",
       "      <td>0.008176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.766890</td>\n",
       "      <td>0.792777</td>\n",
       "      <td>0.868123</td>\n",
       "      <td>0.884475</td>\n",
       "      <td>tweet_topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>large_debate</td>\n",
       "      <td>0.754525</td>\n",
       "      <td>0.964206</td>\n",
       "      <td>0.964669</td>\n",
       "      <td>0.033014</td>\n",
       "      <td>0.004974</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.721511</td>\n",
       "      <td>0.787539</td>\n",
       "      <td>0.959231</td>\n",
       "      <td>0.969180</td>\n",
       "      <td>0.959771</td>\n",
       "      <td>0.969567</td>\n",
       "      <td>yahootopics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>base_modern</td>\n",
       "      <td>0.885590</td>\n",
       "      <td>0.957399</td>\n",
       "      <td>0.958455</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.005395</td>\n",
       "      <td>0.005135</td>\n",
       "      <td>0.871590</td>\n",
       "      <td>0.899590</td>\n",
       "      <td>0.952004</td>\n",
       "      <td>0.962794</td>\n",
       "      <td>0.953320</td>\n",
       "      <td>0.963589</td>\n",
       "      <td>agnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>base_modern</td>\n",
       "      <td>0.905111</td>\n",
       "      <td>0.952650</td>\n",
       "      <td>0.952624</td>\n",
       "      <td>0.011086</td>\n",
       "      <td>0.005539</td>\n",
       "      <td>0.005545</td>\n",
       "      <td>0.894025</td>\n",
       "      <td>0.916197</td>\n",
       "      <td>0.947111</td>\n",
       "      <td>0.958189</td>\n",
       "      <td>0.947079</td>\n",
       "      <td>0.958169</td>\n",
       "      <td>amazonpolarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>base_modern</td>\n",
       "      <td>0.589962</td>\n",
       "      <td>0.920188</td>\n",
       "      <td>0.935362</td>\n",
       "      <td>0.031649</td>\n",
       "      <td>0.007522</td>\n",
       "      <td>0.005491</td>\n",
       "      <td>0.558313</td>\n",
       "      <td>0.621611</td>\n",
       "      <td>0.912666</td>\n",
       "      <td>0.927710</td>\n",
       "      <td>0.929871</td>\n",
       "      <td>0.940853</td>\n",
       "      <td>emotiondair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>base_modern</td>\n",
       "      <td>0.573759</td>\n",
       "      <td>0.808429</td>\n",
       "      <td>0.808559</td>\n",
       "      <td>0.026664</td>\n",
       "      <td>0.012242</td>\n",
       "      <td>0.012286</td>\n",
       "      <td>0.547095</td>\n",
       "      <td>0.600423</td>\n",
       "      <td>0.796187</td>\n",
       "      <td>0.820671</td>\n",
       "      <td>0.796273</td>\n",
       "      <td>0.820845</td>\n",
       "      <td>go_emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>base_modern</td>\n",
       "      <td>0.810951</td>\n",
       "      <td>0.906274</td>\n",
       "      <td>0.906313</td>\n",
       "      <td>0.015822</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>0.007844</td>\n",
       "      <td>0.795129</td>\n",
       "      <td>0.826773</td>\n",
       "      <td>0.898424</td>\n",
       "      <td>0.914123</td>\n",
       "      <td>0.898470</td>\n",
       "      <td>0.914157</td>\n",
       "      <td>rottentomatoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>base_modern</td>\n",
       "      <td>0.785589</td>\n",
       "      <td>0.892726</td>\n",
       "      <td>0.891989</td>\n",
       "      <td>0.016620</td>\n",
       "      <td>0.008352</td>\n",
       "      <td>0.008461</td>\n",
       "      <td>0.768969</td>\n",
       "      <td>0.802209</td>\n",
       "      <td>0.884374</td>\n",
       "      <td>0.901078</td>\n",
       "      <td>0.883528</td>\n",
       "      <td>0.900451</td>\n",
       "      <td>sst2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>base_modern</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.747456</td>\n",
       "      <td>0.855479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013712</td>\n",
       "      <td>0.008987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.733744</td>\n",
       "      <td>0.761168</td>\n",
       "      <td>0.846492</td>\n",
       "      <td>0.864466</td>\n",
       "      <td>tweet_topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>base_modern</td>\n",
       "      <td>0.634182</td>\n",
       "      <td>0.938106</td>\n",
       "      <td>0.942349</td>\n",
       "      <td>0.037617</td>\n",
       "      <td>0.006851</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.596565</td>\n",
       "      <td>0.671800</td>\n",
       "      <td>0.931255</td>\n",
       "      <td>0.944957</td>\n",
       "      <td>0.936182</td>\n",
       "      <td>0.948515</td>\n",
       "      <td>yahootopics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>large_modern</td>\n",
       "      <td>0.875357</td>\n",
       "      <td>0.952167</td>\n",
       "      <td>0.953646</td>\n",
       "      <td>0.014614</td>\n",
       "      <td>0.005949</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.860742</td>\n",
       "      <td>0.889971</td>\n",
       "      <td>0.946218</td>\n",
       "      <td>0.958116</td>\n",
       "      <td>0.948046</td>\n",
       "      <td>0.959247</td>\n",
       "      <td>agnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>large_modern</td>\n",
       "      <td>0.956075</td>\n",
       "      <td>0.978092</td>\n",
       "      <td>0.978088</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>0.003920</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.948217</td>\n",
       "      <td>0.963933</td>\n",
       "      <td>0.974172</td>\n",
       "      <td>0.982012</td>\n",
       "      <td>0.974166</td>\n",
       "      <td>0.982009</td>\n",
       "      <td>amazonpolarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>large_modern</td>\n",
       "      <td>0.547897</td>\n",
       "      <td>0.899844</td>\n",
       "      <td>0.921364</td>\n",
       "      <td>0.029706</td>\n",
       "      <td>0.008440</td>\n",
       "      <td>0.005837</td>\n",
       "      <td>0.518191</td>\n",
       "      <td>0.577602</td>\n",
       "      <td>0.891404</td>\n",
       "      <td>0.908283</td>\n",
       "      <td>0.915527</td>\n",
       "      <td>0.927201</td>\n",
       "      <td>emotiondair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>large_modern</td>\n",
       "      <td>0.693058</td>\n",
       "      <td>0.858238</td>\n",
       "      <td>0.859684</td>\n",
       "      <td>0.023485</td>\n",
       "      <td>0.010968</td>\n",
       "      <td>0.010774</td>\n",
       "      <td>0.669573</td>\n",
       "      <td>0.716543</td>\n",
       "      <td>0.847269</td>\n",
       "      <td>0.869206</td>\n",
       "      <td>0.848910</td>\n",
       "      <td>0.870457</td>\n",
       "      <td>go_emotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>large_modern</td>\n",
       "      <td>0.861443</td>\n",
       "      <td>0.931217</td>\n",
       "      <td>0.931019</td>\n",
       "      <td>0.013652</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>0.006828</td>\n",
       "      <td>0.847791</td>\n",
       "      <td>0.875096</td>\n",
       "      <td>0.924425</td>\n",
       "      <td>0.938009</td>\n",
       "      <td>0.924192</td>\n",
       "      <td>0.937847</td>\n",
       "      <td>rottentomatoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>large_modern</td>\n",
       "      <td>0.844740</td>\n",
       "      <td>0.919177</td>\n",
       "      <td>0.918148</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.007027</td>\n",
       "      <td>0.007196</td>\n",
       "      <td>0.832045</td>\n",
       "      <td>0.857436</td>\n",
       "      <td>0.912150</td>\n",
       "      <td>0.926204</td>\n",
       "      <td>0.910952</td>\n",
       "      <td>0.925343</td>\n",
       "      <td>sst2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>large_modern</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.832562</td>\n",
       "      <td>0.908632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011528</td>\n",
       "      <td>0.006865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821035</td>\n",
       "      <td>0.844090</td>\n",
       "      <td>0.901767</td>\n",
       "      <td>0.915497</td>\n",
       "      <td>tweet_topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>large_modern</td>\n",
       "      <td>0.687733</td>\n",
       "      <td>0.950783</td>\n",
       "      <td>0.952937</td>\n",
       "      <td>0.036609</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>0.005558</td>\n",
       "      <td>0.651124</td>\n",
       "      <td>0.724342</td>\n",
       "      <td>0.944876</td>\n",
       "      <td>0.956690</td>\n",
       "      <td>0.947379</td>\n",
       "      <td>0.958495</td>\n",
       "      <td>yahootopics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model       MCC  Accuracy        F1    MCC_SE  Accuracy_SE  \\\n",
       "0       base_nli  0.837733  0.924664  0.924041  0.005686     0.002667   \n",
       "1      large_nli  0.815963  0.914448  0.913286  0.005966     0.002807   \n",
       "2    base_debate  0.812598  0.912680  0.912558  0.005875     0.002738   \n",
       "3   large_debate  0.845425  0.928200  0.927611  0.005562     0.002610   \n",
       "4    base_modern  0.777288  0.896670  0.896180  0.006534     0.003025   \n",
       "5   large_modern  0.827164  0.919556  0.919379  0.005966     0.002772   \n",
       "0       base_nli  0.900427  0.964126  0.964741  0.014383     0.005234   \n",
       "1       base_nli  0.947675  0.973852  0.973859  0.008174     0.004094   \n",
       "2       base_nli  0.735740  0.960876  0.965640  0.032763     0.005433   \n",
       "3       base_nli  0.766265  0.894636  0.894847  0.020706     0.009403   \n",
       "4       base_nli  0.910391  0.955404  0.955448  0.011091     0.005522   \n",
       "5       base_nli  0.933354  0.966936  0.966910  0.009725     0.004830   \n",
       "6       base_nli  0.000000  0.657724  0.793527  0.000000     0.014220   \n",
       "7       base_nli  0.747604  0.964206  0.964206  0.035649     0.004978   \n",
       "8      large_nli  0.926942  0.973842  0.974220  0.011772     0.004254   \n",
       "9      large_nli  0.966069  0.983039  0.983043  0.006827     0.003416   \n",
       "10     large_nli  0.687366  0.949139  0.956593  0.033173     0.006147   \n",
       "11     large_nli  0.776389  0.899425  0.899527  0.020746     0.009459   \n",
       "12     large_nli  0.919530  0.959940  0.959979  0.010268     0.005131   \n",
       "13     large_nli  0.931839  0.966201  0.966182  0.010121     0.005031   \n",
       "14     large_nli  0.000000  0.529140  0.692075  0.000000     0.015944   \n",
       "15     large_nli  0.809161  0.974646  0.973384  0.031495     0.004251   \n",
       "16   base_debate  0.926206  0.973842  0.974163  0.012312     0.004395   \n",
       "17   base_debate  0.895334  0.947703  0.947654  0.011989     0.006002   \n",
       "18   base_debate  0.544559  0.898279  0.920289  0.028668     0.008051   \n",
       "19   base_debate  0.682917  0.853448  0.854981  0.023583     0.010923   \n",
       "20   base_debate  0.765076  0.883598  0.883112  0.017637     0.008798   \n",
       "21   base_debate  0.792594  0.895665  0.894794  0.016095     0.008285   \n",
       "22   base_debate  0.000000  0.884366  0.938635  0.000000     0.009539   \n",
       "23   base_debate  0.650251  0.943326  0.946393  0.036021     0.006148   \n",
       "24  large_debate  0.926567  0.973842  0.974192  0.011797     0.004297   \n",
       "25  large_debate  0.954652  0.977385  0.977382  0.008036     0.004015   \n",
       "26  large_debate  0.670214  0.944444  0.953072  0.030677     0.006040   \n",
       "27  large_debate  0.731472  0.879310  0.879392  0.021973     0.009908   \n",
       "28  large_debate  0.861807  0.930461  0.930021  0.014169     0.007290   \n",
       "29  large_debate  0.871802  0.934607  0.934111  0.012575     0.006662   \n",
       "30  large_debate  0.000000  0.779833  0.876299  0.000000     0.012944   \n",
       "31  large_debate  0.754525  0.964206  0.964669  0.033014     0.004974   \n",
       "32   base_modern  0.885590  0.957399  0.958455  0.014000     0.005395   \n",
       "33   base_modern  0.905111  0.952650  0.952624  0.011086     0.005539   \n",
       "34   base_modern  0.589962  0.920188  0.935362  0.031649     0.007522   \n",
       "35   base_modern  0.573759  0.808429  0.808559  0.026664     0.012242   \n",
       "36   base_modern  0.810951  0.906274  0.906313  0.015822     0.007850   \n",
       "37   base_modern  0.785589  0.892726  0.891989  0.016620     0.008352   \n",
       "38   base_modern  0.000000  0.747456  0.855479  0.000000     0.013712   \n",
       "39   base_modern  0.634182  0.938106  0.942349  0.037617     0.006851   \n",
       "40  large_modern  0.875357  0.952167  0.953646  0.014614     0.005949   \n",
       "41  large_modern  0.956075  0.978092  0.978088  0.007858     0.003920   \n",
       "42  large_modern  0.547897  0.899844  0.921364  0.029706     0.008440   \n",
       "43  large_modern  0.693058  0.858238  0.859684  0.023485     0.010968   \n",
       "44  large_modern  0.861443  0.931217  0.931019  0.013652     0.006792   \n",
       "45  large_modern  0.844740  0.919177  0.918148  0.012696     0.007027   \n",
       "46  large_modern  0.000000  0.832562  0.908632  0.000000     0.011528   \n",
       "47  large_modern  0.687733  0.950783  0.952937  0.036609     0.005907   \n",
       "\n",
       "       F1_SE  MCC_Lower  MCC_Upper  Accuracy_Lower  Accuracy_Upper  F1_Lower  \\\n",
       "0   0.002708   0.832046   0.843419        0.921997        0.927330  0.921334   \n",
       "1   0.002878   0.809997   0.821929        0.911642        0.917255  0.910408   \n",
       "2   0.002749   0.806722   0.818473        0.909942        0.915419  0.909809   \n",
       "3   0.002651   0.839863   0.850986        0.925590        0.930809  0.924961   \n",
       "4   0.003055   0.770755   0.783822        0.893645        0.899696  0.893125   \n",
       "5   0.002781   0.821197   0.833130        0.916784        0.922328  0.916597   \n",
       "0   0.005059   0.886045   0.914810        0.958892        0.969359  0.959682   \n",
       "1   0.004092   0.939501   0.955849        0.969758        0.977946  0.969768   \n",
       "2   0.004310   0.702977   0.768502        0.955443        0.966310  0.961330   \n",
       "3   0.009359   0.745559   0.786971        0.885233        0.904039  0.885488   \n",
       "4   0.005511   0.899300   0.921482        0.949883        0.960926  0.949938   \n",
       "5   0.004837   0.923628   0.943079        0.962106        0.971766  0.962072   \n",
       "6   0.010345   0.000000   0.000000        0.643504        0.671945  0.783181   \n",
       "7   0.005019   0.711955   0.783253        0.959227        0.969184  0.959187   \n",
       "8   0.004137   0.915169   0.938714        0.969587        0.978096  0.970083   \n",
       "9   0.003414   0.959242   0.972896        0.979623        0.986455  0.979629   \n",
       "10  0.004696   0.654193   0.720539        0.942992        0.955287  0.951896   \n",
       "11  0.009431   0.755643   0.797135        0.889967        0.908884  0.890096   \n",
       "12  0.005121   0.909262   0.929798        0.954809        0.965070  0.954858   \n",
       "13  0.005036   0.921718   0.941959        0.961170        0.971233  0.961146   \n",
       "14  0.013635   0.000000   0.000000        0.513196        0.545084  0.678440   \n",
       "15  0.004659   0.777666   0.840657        0.970395        0.978896  0.968726   \n",
       "16  0.004285   0.913895   0.938518        0.969447        0.978236  0.969878   \n",
       "17  0.006012   0.883345   0.907323        0.941701        0.953705  0.941642   \n",
       "18  0.005657   0.515891   0.573227        0.890228        0.906329  0.914632   \n",
       "19  0.010727   0.659334   0.706500        0.842526        0.864371  0.844254   \n",
       "20  0.008867   0.747440   0.782713        0.874800        0.892395  0.874245   \n",
       "21  0.008421   0.776499   0.808689        0.887380        0.903950  0.886374   \n",
       "22  0.005377   0.000000   0.000000        0.874828        0.893905  0.933258   \n",
       "23  0.005637   0.614230   0.686271        0.937178        0.949474  0.940755   \n",
       "24  0.004182   0.914770   0.938364        0.969544        0.978139  0.970010   \n",
       "25  0.004016   0.946616   0.962687        0.973371        0.981400  0.973365   \n",
       "26  0.004578   0.639538   0.700891        0.938404        0.950485  0.948494   \n",
       "27  0.009898   0.709499   0.753444        0.869403        0.889218  0.869494   \n",
       "28  0.007376   0.847639   0.875976        0.923171        0.937751  0.922645   \n",
       "29  0.006761   0.859226   0.884377        0.927945        0.941269  0.927349   \n",
       "30  0.008176   0.000000   0.000000        0.766890        0.792777  0.868123   \n",
       "31  0.004898   0.721511   0.787539        0.959231        0.969180  0.959771   \n",
       "32  0.005135   0.871590   0.899590        0.952004        0.962794  0.953320   \n",
       "33  0.005545   0.894025   0.916197        0.947111        0.958189  0.947079   \n",
       "34  0.005491   0.558313   0.621611        0.912666        0.927710  0.929871   \n",
       "35  0.012286   0.547095   0.600423        0.796187        0.820671  0.796273   \n",
       "36  0.007844   0.795129   0.826773        0.898424        0.914123  0.898470   \n",
       "37  0.008461   0.768969   0.802209        0.884374        0.901078  0.883528   \n",
       "38  0.008987   0.000000   0.000000        0.733744        0.761168  0.846492   \n",
       "39  0.006167   0.596565   0.671800        0.931255        0.944957  0.936182   \n",
       "40  0.005600   0.860742   0.889971        0.946218        0.958116  0.948046   \n",
       "41  0.003922   0.948217   0.963933        0.974172        0.982012  0.974166   \n",
       "42  0.005837   0.518191   0.577602        0.891404        0.908283  0.915527   \n",
       "43  0.010774   0.669573   0.716543        0.847269        0.869206  0.848910   \n",
       "44  0.006828   0.847791   0.875096        0.924425        0.938009  0.924192   \n",
       "45  0.007196   0.832045   0.857436        0.912150        0.926204  0.910952   \n",
       "46  0.006865   0.000000   0.000000        0.821035        0.844090  0.901767   \n",
       "47  0.005558   0.651124   0.724342        0.944876        0.956690  0.947379   \n",
       "\n",
       "    F1_Upper            Task  \n",
       "0   0.926749         overall  \n",
       "1   0.916164         overall  \n",
       "2   0.915307         overall  \n",
       "3   0.930262         overall  \n",
       "4   0.899235         overall  \n",
       "5   0.922160         overall  \n",
       "0   0.969800          agnews  \n",
       "1   0.977951  amazonpolarity  \n",
       "2   0.969950     emotiondair  \n",
       "3   0.904206     go_emotions  \n",
       "4   0.960959  rottentomatoes  \n",
       "5   0.971747            sst2  \n",
       "6   0.803872     tweet_topic  \n",
       "7   0.969225     yahootopics  \n",
       "8   0.978357          agnews  \n",
       "9   0.986457  amazonpolarity  \n",
       "10  0.961289     emotiondair  \n",
       "11  0.908958     go_emotions  \n",
       "12  0.965100  rottentomatoes  \n",
       "13  0.971218            sst2  \n",
       "14  0.705710     tweet_topic  \n",
       "15  0.978043     yahootopics  \n",
       "16  0.978448          agnews  \n",
       "17  0.953667  amazonpolarity  \n",
       "18  0.925945     emotiondair  \n",
       "19  0.865708     go_emotions  \n",
       "20  0.891979  rottentomatoes  \n",
       "21  0.903215            sst2  \n",
       "22  0.944012     tweet_topic  \n",
       "23  0.952030     yahootopics  \n",
       "24  0.978374          agnews  \n",
       "25  0.981398  amazonpolarity  \n",
       "26  0.957649     emotiondair  \n",
       "27  0.889290     go_emotions  \n",
       "28  0.937398  rottentomatoes  \n",
       "29  0.940872            sst2  \n",
       "30  0.884475     tweet_topic  \n",
       "31  0.969567     yahootopics  \n",
       "32  0.963589          agnews  \n",
       "33  0.958169  amazonpolarity  \n",
       "34  0.940853     emotiondair  \n",
       "35  0.820845     go_emotions  \n",
       "36  0.914157  rottentomatoes  \n",
       "37  0.900451            sst2  \n",
       "38  0.864466     tweet_topic  \n",
       "39  0.948515     yahootopics  \n",
       "40  0.959247          agnews  \n",
       "41  0.982009  amazonpolarity  \n",
       "42  0.927201     emotiondair  \n",
       "43  0.870457     go_emotions  \n",
       "44  0.937847  rottentomatoes  \n",
       "45  0.925343            sst2  \n",
       "46  0.915497     tweet_topic  \n",
       "47  0.958495     yahootopics  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall.reset_index(inplace = True)\n",
    "overall['Task'] = 'overall'\n",
    "task.reset_index(inplace = True)\n",
    "\n",
    "combined = pd.concat([overall, task])\n",
    "combined.rename({'Column':'Model'}, axis = 1, inplace = True)\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce13b09a-afcd-4896-b9e6-ff8a1e439189",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "48aac652-3974-426e-81ce-e4d7c1b49f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.replace({'base_nli': 'NLI Base', \n",
    "                'large_nli':'NLI Large', \n",
    "                'base_debate':'DEBATE Base',\n",
    "                'large_debate':'DEBATE Large',\n",
    "                'base_modern': 'DEBATE Base (MB)',\n",
    "                'large_modern': 'DEBATE Large (MB)',\n",
    "                'tweet_topic': 'Tweet Topics',\n",
    "                'yahootopics': 'Yahoo Topics',\n",
    "                'amazonpolarity': 'Amazon Polarity',\n",
    "                'rottentomatoes': 'Rotten Tomatoes',\n",
    "                'agnews': 'AG News',\n",
    "                'emotiondair': 'DAIR AI Emotions',\n",
    "                'sst2': 'Stanford Sentiment 2',\n",
    "                'go_emotions': 'Google Emotions',\n",
    "                'overall': 'Overall'\n",
    "           }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "42c29f28-3688-4b05-8efb-7829a189ad84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined['F1'] = combined['F1'] * 100\n",
    "combined['F1_SE'] = combined['F1_SE'] * 100\n",
    "\n",
    "combined['Accuracy'] = combined['Accuracy'] * 100\n",
    "combined['Accuracy_SE'] = combined['Accuracy_SE'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a2b99532-c078-4d23-ad31-a918a8147b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllll}\n",
      "\\toprule\n",
      "Model & AG News & Tweet Topics & Yahoo Topics & Amazon Polarity & Rotten Tomatoes & Stanford Sentiment 2 & DAIR AI Emotions & Google Emotions & Overall \\\\\n",
      "\\midrule\n",
      "NLI Base & $96.5 \\ (0.5)$ & $79.4 \\ (1.0)$ & $96.4 \\ (0.5)$ & $97.4 \\ (0.4)$ & $95.5 \\ (0.6)$ & $\\mathbf{96.7} \\ (0.5)$ & $\\mathbf{96.6} \\ (0.4)$ & $89.5 \\ (0.9)$ & $92.4 \\ (0.3)$ \\\\\n",
      "NLI Large & $\\mathbf{97.4} \\ (0.4)$ & $69.2 \\ (1.4)$ & $\\mathbf{97.3} \\ (0.5)$ & $\\mathbf{98.3} \\ (0.3)$ & $\\mathbf{96.0} \\ (0.5)$ & $96.6 \\ (0.5)$ & $95.7 \\ (0.5)$ & $\\mathbf{90.0} \\ (0.9)$ & $91.3 \\ (0.3)$ \\\\\n",
      "DEBATE Base (MB) & $95.8 \\ (0.5)$ & $85.5 \\ (0.9)$ & $94.2 \\ (0.6)$ & $95.3 \\ (0.6)$ & $90.6 \\ (0.8)$ & $89.2 \\ (0.8)$ & $93.5 \\ (0.5)$ & $80.9 \\ (1.2)$ & $89.6 \\ (0.3)$ \\\\\n",
      "DEBATE Large (MB) & $95.4 \\ (0.6)$ & $90.9 \\ (0.7)$ & $95.3 \\ (0.6)$ & $97.8 \\ (0.4)$ & $93.1 \\ (0.7)$ & $91.8 \\ (0.7)$ & $92.1 \\ (0.6)$ & $86.0 \\ (1.1)$ & $91.9 \\ (0.3)$ \\\\\n",
      "DEBATE Base & $97.4 \\ (0.4)$ & $\\mathbf{93.9} \\ (0.5)$ & $94.6 \\ (0.6)$ & $94.8 \\ (0.6)$ & $88.3 \\ (0.9)$ & $89.5 \\ (0.8)$ & $92.0 \\ (0.6)$ & $85.5 \\ (1.1)$ & $91.3 \\ (0.3)$ \\\\\n",
      "DEBATE Large & $97.4 \\ (0.4)$ & $87.6 \\ (0.8)$ & $96.5 \\ (0.5)$ & $97.7 \\ (0.4)$ & $93.0 \\ (0.7)$ & $93.4 \\ (0.7)$ & $95.3 \\ (0.5)$ & $87.9 \\ (1.0)$ & $\\mathbf{92.8} \\ (0.3)$ \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pivot table for F1 and F1_SE\n",
    "f1_table = combined.pivot(index='Model', columns='Task', values=['F1', 'F1_SE'])\n",
    "f1_table = f1_table.reindex(['NLI Base', 'NLI Large', 'DEBATE Base (MB)', 'DEBATE Large (MB)', 'DEBATE Base', 'DEBATE Large'])\n",
    "\n",
    "cols = [(   'F1', 'AG News'),\n",
    "        (   'F1', 'Tweet Topics'),\n",
    "        (   'F1', 'Yahoo Topics'),\n",
    "        (   'F1', 'Amazon Polarity'),\n",
    "        (   'F1', 'Rotten Tomatoes'),\n",
    "        (   'F1', 'Stanford Sentiment 2'),\n",
    "        (   'F1', 'DAIR AI Emotions'),\n",
    "        (   'F1', 'Google Emotions'),\n",
    "        (   'F1', 'Overall'),\n",
    "        ('F1_SE', 'AG News'),\n",
    "        ('F1_SE', 'Tweet Topics'),\n",
    "        ('F1_SE', 'Yahoo Topics'),\n",
    "        ('F1_SE', 'Amazon Polarity'),\n",
    "        ('F1_SE', 'Rotten Tomatoes'),\n",
    "        ('F1_SE', 'Stanford Sentiment 2'),\n",
    "        ('F1_SE', 'DAIR AI Emotions'),\n",
    "        ('F1_SE', 'Google Emotions'),\n",
    "        ('F1_SE', 'Overall')\n",
    "]\n",
    "\n",
    "f1_table = f1_table.reindex(cols, axis = 1)\n",
    "\n",
    "# Identify the largest F1 scores per task\n",
    "max_f1_per_task = f1_table['F1'].idxmax()\n",
    "\n",
    "# Format the F1 scores with standard errors and highlight the max values\n",
    "def format_f1_table(f1_table, max_index):\n",
    "    # Initialize the formatted table with string type\n",
    "    formatted_table = f1_table['F1'].copy().astype(str)\n",
    "    for col in f1_table['F1'].columns:  # Iterate over each task (column in F1)\n",
    "        max_model = max_index[col]  # Get the model with the maximum F1 score\n",
    "        for idx in f1_table.index:\n",
    "            f1 = f1_table.at[idx, ('F1', col)]\n",
    "            se = f1_table.at[idx, ('F1_SE', col)]\n",
    "            if idx == max_model:\n",
    "                # Bold the maximum F1 score\n",
    "                formatted_table.at[idx, col] = f\"$\\\\mathbf{{{f1:.1f}}} \\ ({se:.1f})$\"\n",
    "            else:\n",
    "                # Format other scores without bold\n",
    "                formatted_table.at[idx, col] = f\"${f1:.1f} \\ ({se:.1f})$\"\n",
    "    return formatted_table\n",
    "\n",
    "# Apply the formatting function\n",
    "formatted_f1_table = format_f1_table(f1_table, max_f1_per_task)\n",
    "\n",
    "# Reset index for better formatting\n",
    "formatted_f1_table = formatted_f1_table.reset_index()\n",
    "\n",
    "# Convert to LaTeX\n",
    "latex_table = formatted_f1_table.to_latex(index=False, escape=False, multicolumn=True, float_format=\"%.1f\")\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c419250c-9791-4adf-9641-0a33efe232ab",
   "metadata": {},
   "source": [
    "Difference plot for models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8f1bb183-5b9b-4afb-955e-6beb93b721d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Task</th>\n",
       "      <th>Model</th>\n",
       "      <th>AG News</th>\n",
       "      <th>Tweet Topics</th>\n",
       "      <th>Yahoo Topics</th>\n",
       "      <th>Amazon Polarity</th>\n",
       "      <th>Rotten Tomatoes</th>\n",
       "      <th>Stanford Sentiment 2</th>\n",
       "      <th>DAIR AI Emotions</th>\n",
       "      <th>Google Emotions</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NLI Base</td>\n",
       "      <td>$96.5 \\ (0.5)$</td>\n",
       "      <td>$79.4 \\ (1.0)$</td>\n",
       "      <td>$96.4 \\ (0.5)$</td>\n",
       "      <td>$97.4 \\ (0.4)$</td>\n",
       "      <td>$95.5 \\ (0.6)$</td>\n",
       "      <td>$\\mathbf{96.7} \\ (0.5)$</td>\n",
       "      <td>$\\mathbf{96.6} \\ (0.4)$</td>\n",
       "      <td>$89.5 \\ (0.9)$</td>\n",
       "      <td>$92.4 \\ (0.3)$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NLI Large</td>\n",
       "      <td>$\\mathbf{97.4} \\ (0.4)$</td>\n",
       "      <td>$69.2 \\ (1.4)$</td>\n",
       "      <td>$\\mathbf{97.3} \\ (0.5)$</td>\n",
       "      <td>$\\mathbf{98.3} \\ (0.3)$</td>\n",
       "      <td>$\\mathbf{96.0} \\ (0.5)$</td>\n",
       "      <td>$96.6 \\ (0.5)$</td>\n",
       "      <td>$95.7 \\ (0.5)$</td>\n",
       "      <td>$\\mathbf{90.0} \\ (0.9)$</td>\n",
       "      <td>$91.3 \\ (0.3)$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEBATE Base (MB)</td>\n",
       "      <td>$95.8 \\ (0.5)$</td>\n",
       "      <td>$85.5 \\ (0.9)$</td>\n",
       "      <td>$94.2 \\ (0.6)$</td>\n",
       "      <td>$95.3 \\ (0.6)$</td>\n",
       "      <td>$90.6 \\ (0.8)$</td>\n",
       "      <td>$89.2 \\ (0.8)$</td>\n",
       "      <td>$93.5 \\ (0.5)$</td>\n",
       "      <td>$80.9 \\ (1.2)$</td>\n",
       "      <td>$89.6 \\ (0.3)$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEBATE Large (MB)</td>\n",
       "      <td>$95.4 \\ (0.6)$</td>\n",
       "      <td>$90.9 \\ (0.7)$</td>\n",
       "      <td>$95.3 \\ (0.6)$</td>\n",
       "      <td>$97.8 \\ (0.4)$</td>\n",
       "      <td>$93.1 \\ (0.7)$</td>\n",
       "      <td>$91.8 \\ (0.7)$</td>\n",
       "      <td>$92.1 \\ (0.6)$</td>\n",
       "      <td>$86.0 \\ (1.1)$</td>\n",
       "      <td>$91.9 \\ (0.3)$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEBATE Base</td>\n",
       "      <td>$97.4 \\ (0.4)$</td>\n",
       "      <td>$\\mathbf{93.9} \\ (0.5)$</td>\n",
       "      <td>$94.6 \\ (0.6)$</td>\n",
       "      <td>$94.8 \\ (0.6)$</td>\n",
       "      <td>$88.3 \\ (0.9)$</td>\n",
       "      <td>$89.5 \\ (0.8)$</td>\n",
       "      <td>$92.0 \\ (0.6)$</td>\n",
       "      <td>$85.5 \\ (1.1)$</td>\n",
       "      <td>$91.3 \\ (0.3)$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DEBATE Large</td>\n",
       "      <td>$97.4 \\ (0.4)$</td>\n",
       "      <td>$87.6 \\ (0.8)$</td>\n",
       "      <td>$96.5 \\ (0.5)$</td>\n",
       "      <td>$97.7 \\ (0.4)$</td>\n",
       "      <td>$93.0 \\ (0.7)$</td>\n",
       "      <td>$93.4 \\ (0.7)$</td>\n",
       "      <td>$95.3 \\ (0.5)$</td>\n",
       "      <td>$87.9 \\ (1.0)$</td>\n",
       "      <td>$\\mathbf{92.8} \\ (0.3)$</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Task              Model                  AG News             Tweet Topics  \\\n",
       "0              NLI Base           $96.5 \\ (0.5)$           $79.4 \\ (1.0)$   \n",
       "1             NLI Large  $\\mathbf{97.4} \\ (0.4)$           $69.2 \\ (1.4)$   \n",
       "2      DEBATE Base (MB)           $95.8 \\ (0.5)$           $85.5 \\ (0.9)$   \n",
       "3     DEBATE Large (MB)           $95.4 \\ (0.6)$           $90.9 \\ (0.7)$   \n",
       "4           DEBATE Base           $97.4 \\ (0.4)$  $\\mathbf{93.9} \\ (0.5)$   \n",
       "5          DEBATE Large           $97.4 \\ (0.4)$           $87.6 \\ (0.8)$   \n",
       "\n",
       "Task             Yahoo Topics          Amazon Polarity  \\\n",
       "0              $96.4 \\ (0.5)$           $97.4 \\ (0.4)$   \n",
       "1     $\\mathbf{97.3} \\ (0.5)$  $\\mathbf{98.3} \\ (0.3)$   \n",
       "2              $94.2 \\ (0.6)$           $95.3 \\ (0.6)$   \n",
       "3              $95.3 \\ (0.6)$           $97.8 \\ (0.4)$   \n",
       "4              $94.6 \\ (0.6)$           $94.8 \\ (0.6)$   \n",
       "5              $96.5 \\ (0.5)$           $97.7 \\ (0.4)$   \n",
       "\n",
       "Task          Rotten Tomatoes     Stanford Sentiment 2  \\\n",
       "0              $95.5 \\ (0.6)$  $\\mathbf{96.7} \\ (0.5)$   \n",
       "1     $\\mathbf{96.0} \\ (0.5)$           $96.6 \\ (0.5)$   \n",
       "2              $90.6 \\ (0.8)$           $89.2 \\ (0.8)$   \n",
       "3              $93.1 \\ (0.7)$           $91.8 \\ (0.7)$   \n",
       "4              $88.3 \\ (0.9)$           $89.5 \\ (0.8)$   \n",
       "5              $93.0 \\ (0.7)$           $93.4 \\ (0.7)$   \n",
       "\n",
       "Task         DAIR AI Emotions          Google Emotions  \\\n",
       "0     $\\mathbf{96.6} \\ (0.4)$           $89.5 \\ (0.9)$   \n",
       "1              $95.7 \\ (0.5)$  $\\mathbf{90.0} \\ (0.9)$   \n",
       "2              $93.5 \\ (0.5)$           $80.9 \\ (1.2)$   \n",
       "3              $92.1 \\ (0.6)$           $86.0 \\ (1.1)$   \n",
       "4              $92.0 \\ (0.6)$           $85.5 \\ (1.1)$   \n",
       "5              $95.3 \\ (0.5)$           $87.9 \\ (1.0)$   \n",
       "\n",
       "Task                  Overall  \n",
       "0              $92.4 \\ (0.3)$  \n",
       "1              $91.3 \\ (0.3)$  \n",
       "2              $89.6 \\ (0.3)$  \n",
       "3              $91.9 \\ (0.3)$  \n",
       "4              $91.3 \\ (0.3)$  \n",
       "5     $\\mathbf{92.8} \\ (0.3)$  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_f1_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e3409-c786-44a2-84ec-83ab346af0cd",
   "metadata": {},
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a4e92-9088-4ec2-a50c-1f8a00e6e951",
   "metadata": {},
   "source": [
    "# NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ac8545ae-7d4f-45b0-a6c7-3125fd58d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nli = pd.read_csv('../data/nli_bench.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "109c770d-1cca-42c9-8d8c-a8bdbc3227bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mlburnham/Political_DEBATE_base_v1.0\"\n",
    "res, metrics = benchmark(model = model, data = Dataset.from_pandas(nli), textcol = 'text', hypcol = 'hypothesis', labelcol = 'labels')\n",
    "nli['debate_base'] = res['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "56efe583-4357-4796-b1c6-2d7fecc78a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'MoritzLaurer/deberta-v3-base-zeroshot-v2.0'\n",
    "res, metrics = benchmark(model = model, data = Dataset.from_pandas(nli), textcol = 'text', hypcol = 'hypothesis', labelcol = 'labels')\n",
    "nli['deberta_base'] = res['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e679ad-e7cd-42ff-8716-e963c193bcd9",
   "metadata": {},
   "source": [
    "## Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "d601293b-9479-4bde-8511-941c3c0e4f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = nli\n",
    "\n",
    "results = []\n",
    "for column in ['debate_base', 'deberta_base']:\n",
    "    metrics = get_metrics(df['labels'], df[column])\n",
    "    metrics_se = bootstrapped_errors(pd.DataFrame({\n",
    "        'entailment': df['labels'],\n",
    "        'label': df[column]\n",
    "    }))\n",
    "    combined_results = {**metrics, **metrics_se, 'Model': column}\n",
    "    results.append(combined_results)\n",
    "\n",
    "# Calculate metrics by task\n",
    "tasks = df['task_name'].unique()\n",
    "for task in tasks:\n",
    "    task_df = df[df['task_name'] == task]\n",
    "    for column in ['debate_base', 'deberta_base']:\n",
    "        metrics = get_metrics(task_df['labels'], task_df[column])\n",
    "        metrics_se = bootstrapped_errors(pd.DataFrame({\n",
    "            'entailment': task_df['labels'],\n",
    "            'label': task_df[column]\n",
    "        }))\n",
    "        combined_results = {**metrics, **metrics_se, 'Model': column, 'Task': task}\n",
    "        results.append(combined_results)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.loc[results_df['Task'].isna(), 'Task'] = 'Overall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "71c8c906-4bec-4eaf-a679-f779424dedba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "Model & Overall & anli_r1 & mnli_m & wanli \\\\\n",
      "\\midrule\n",
      "debate_base & $0.660 \\pm 0.009$ & $0.556 \\pm 0.016$ & $0.771 \\pm 0.013$ & $0.649 \\pm 0.015$ \\\\\n",
      "deberta_base & $\\mathbf{0.857} \\pm 0.006$ & $\\mathbf{0.822} \\pm 0.012$ & $\\mathbf{0.945} \\pm 0.007$ & $\\mathbf{0.804} \\pm 0.012$ \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pivot table for F1 and F1_SE\n",
    "f1_table = results_df.pivot(index='Model', columns='Task', values=['F1', 'F1_SE'])\n",
    "\n",
    "# Identify the largest F1 scores per task\n",
    "max_f1_per_task = f1_table['F1'].idxmax()\n",
    "\n",
    "# Format the F1 scores with standard errors and highlight the max values\n",
    "def format_f1(row, max_index):\n",
    "    return row['F1'].combine(row['F1_SE'], \n",
    "        lambda f1, se, task=row.name: f\"$\\\\mathbf{{{f1:.3f}}} \\\\pm {se:.3f}$\" if task in max_index.values else f\"${f1:.3f} \\\\pm {se:.3f}$\")\n",
    "\n",
    "formatted_f1_table = f1_table.apply(\n",
    "    lambda row: format_f1(row, max_f1_per_task), axis=1\n",
    ")\n",
    "\n",
    "# Reset index for better formatting\n",
    "formatted_f1_table = formatted_f1_table.reset_index()\n",
    "\n",
    "# Convert to LaTeX\n",
    "latex_table = formatted_f1_table.to_latex(index=False, escape=False, multicolumn=True, float_format=\"%.3f\")\n",
    "\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
