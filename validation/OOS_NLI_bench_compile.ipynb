{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7daf1ae-489a-41db-b564-93466826c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53436179-b54b-4ef8-9919-87a135912b4c",
   "metadata": {},
   "source": [
    "# Initial Data Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4e3d914d-946c-4e9b-bbad-8edd052c1ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "zsnli = load_dataset(\"MoritzLaurer/zeroshot_test_downsampled\")\n",
    "\n",
    "drop = ['mnli_mm', 'fevernli', 'anli_r2', 'anli_r3', 'lingnli', 'imdb', 'yelpreviews', 'hatexplain',\n",
    " 'emocontext', 'empathetic', 'biasframes_sex', 'biasframes_offensive', 'biasframes_intent', 'financialphrasebank', \n",
    " 'appreviews', 'hateoffensive', 'trueteacher', 'spam', 'wikitoxic_toxicaggregated', 'wikitoxic_obscene', \n",
    " 'wikitoxic_identityhate', 'wikitoxic_threat', 'wikitoxic_insult', 'wellformedquery', 'massive', 'banking77', \n",
    " 'manifesto', 'capsotu']\n",
    "for data in drop:\n",
    "    zsnli.pop(data)\n",
    "\n",
    "# download additional datasets\n",
    "sst2 = load_dataset(\"SetFit/sst2\")\n",
    "tweet_topic = load_dataset(\"cardiffnlp/tweet_topic_single\")\n",
    "go_emotions = load_dataset(\"SetFit/go_emotions\")\n",
    "\n",
    "# Add to the dataset dict\n",
    "zsnli['sst2'] = sst2['test']\n",
    "zsnli['tweet_topic'] = tweet_topic['test_2021']\n",
    "zsnli['go_emotions'] = go_emotions['validation']\n",
    "\n",
    "# create a dict with only the the nli datasets\n",
    "nli_datasets = ['mnli_m', 'anli_r1', 'wanli']\n",
    "nli = DatasetDict({key: zsnli[key] for key in nli_datasets})\n",
    "for data in nli_datasets:\n",
    "    zsnli.pop(data)\n",
    "    \n",
    "# Create a new DatasetDict with the random samples\n",
    "# 1000 for the nli datasets since those wont be validated\n",
    "sample_size = 1000\n",
    "nli = DatasetDict({\n",
    "    key: dataset.shuffle(seed=1).select(range(min(sample_size, len(dataset))))\n",
    "    for key, dataset in nli.items()\n",
    "})\n",
    "\n",
    "# 1500 for the other datasets to account for possible loss during validation\n",
    "sample_size = 1500\n",
    "zsnli = DatasetDict({\n",
    "    key: dataset.shuffle(seed=1).select(range(min(sample_size, len(dataset))))\n",
    "    for key, dataset in zsnli.items()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb67733-613a-4ad1-9770-995fb15d536f",
   "metadata": {},
   "source": [
    "# SST2 Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2e3b02ab-0e62-43f2-9b9e-94595daa4104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean sst2\n",
    "sst2 = zsnli['sst2'].to_pandas()\n",
    "\n",
    "# add entailment hypotheses\n",
    "sst2['hypothesis'] = \"The sentiment of this text is positive\"\n",
    "sst2.loc[750:,'hypothesis'] = \"The sentiment of this text is negative\"\n",
    "\n",
    "# add entailment labels\n",
    "sst2['labels'] = 0\n",
    "\n",
    "pos_index = sst2[sst2['label_text'] == 'positive'].index\n",
    "pos_labels = [1 if 'negative' in text else 0 for text in sst2.loc[pos_index, 'hypothesis']]\n",
    "sst2.loc[pos_index, 'labels'] = pos_labels\n",
    "\n",
    "neg_index = sst2[sst2['label_text'] == 'negative'].index\n",
    "neg_labels = [1 if 'positive' in text else 0 for text in sst2.loc[neg_index, 'hypothesis']]\n",
    "sst2.loc[neg_index, 'labels'] = neg_labels\n",
    "\n",
    "# add task name\n",
    "sst2['task_name'] = 'sst2'\n",
    "\n",
    "# add label text\n",
    "sst2['label_text'] = ['entailment' if label == 0 else 'not_entailment' for label in sst2['labels']]\n",
    "\n",
    "# drop original label column\n",
    "sst2.drop('label', axis = 1, inplace = True)\n",
    "\n",
    "# re-order\n",
    "sst2 = sst2[['text', 'hypothesis', 'labels', 'task_name', 'label_text']]\n",
    "\n",
    "zsnli['sst2'] = Dataset.from_pandas(sst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42421bd-fc07-4466-8a50-cf9d5b56b71b",
   "metadata": {},
   "source": [
    "# Tweets Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "12352002-b6cb-4b3f-a2af-786a9cbda8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1q/xggcl4hn6mx_q8dfxbx_whpr0000gq/T/ipykernel_59961/857001129.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tweets['label_name'].replace(hypoths, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "# Clean the tweets topic data\n",
    "tweets = zsnli['tweet_topic'].to_pandas()\n",
    "\n",
    "hypoths = {'sports_&_gaming': 'This text is about sports and gaming', \n",
    "           'daily_life': 'This text is about daily life',\n",
    "           'pop_culture': 'This text is about pop culture', \n",
    "           'science_&_technology': 'This text is about science and technology',\n",
    "           'business_&_entreprenuers': 'This text is about business and entreprenuers', \n",
    "           'arts_&_culture': 'This text is about arts and culture'}\n",
    "\n",
    "tweets['label_name'].replace(hypoths, inplace = True)\n",
    "tweets['labels'] = 0\n",
    "tweets.drop(['date', 'label', 'id'], axis = 1, inplace = True)\n",
    "tweets['task_name'] = 'tweet_topic'\n",
    "tweets['label_text'] = 'entailment'\n",
    "tweets.rename({'label_name':'hypothesis'}, axis = 1, inplace = True)\n",
    "zsnli['tweet_topic'] = Dataset.from_pandas(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee63fe6-e4da-4b62-95f5-a50325fe733d",
   "metadata": {},
   "source": [
    "# Emotion Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "069f9637-5a60-4fbd-a6fb-e2be52f1ceb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1q/xggcl4hn6mx_q8dfxbx_whpr0000gq/T/ipykernel_59961/3103870865.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  emo['hypothesis'].replace({'This text expresses neutral.': 'This text does not express emotion.'}, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "emo = zsnli['go_emotions'].to_pandas()\n",
    "\n",
    "emotion_columns = [col for col in emo.columns if col != 'text']\n",
    "\n",
    "# Initialize lists to store the results\n",
    "texts = []\n",
    "hypotheses = []\n",
    "labels = []\n",
    "label_text = []\n",
    "\n",
    "# Split the DataFrame into two halves\n",
    "midpoint = len(emo) // 2\n",
    "true_hypothesis_df = emo.iloc[:midpoint]\n",
    "false_hypothesis_df = emo.iloc[midpoint:]\n",
    "\n",
    "# Create true hypotheses\n",
    "for _, row in true_hypothesis_df.iterrows():\n",
    "    # Get the expressed emotions for this text\n",
    "    expressed_emotions = [emotion for emotion in emotion_columns if row[emotion] == 1]\n",
    "    if expressed_emotions:\n",
    "        # Randomly select one expressed emotion\n",
    "        selected_emotion = random.choice(expressed_emotions)\n",
    "        texts.append(row['text'])\n",
    "        hypotheses.append(f\"This text expresses {selected_emotion}.\")\n",
    "        labels.append(0)\n",
    "        label_text.append(\"entailment\")\n",
    "\n",
    "# Create false hypotheses\n",
    "for _, row in false_hypothesis_df.iterrows():\n",
    "    # Get the emotions not expressed in this text\n",
    "    non_expressed_emotions = [emotion for emotion in emotion_columns if row[emotion] == 0]\n",
    "    if non_expressed_emotions:\n",
    "        # Randomly select one non-expressed emotion\n",
    "        selected_emotion = random.choice(non_expressed_emotions)\n",
    "        texts.append(row['text'])\n",
    "        hypotheses.append(f\"This text expresses {selected_emotion}.\")\n",
    "        labels.append(1)\n",
    "        label_text.append(\"not_entailment\")\n",
    "\n",
    "# Create a new DataFrame with the entailment pairs and labels\n",
    "emo = pd.DataFrame({\n",
    "    \"text\": texts,\n",
    "    \"hypothesis\": hypotheses,\n",
    "    \"labels\": labels,\n",
    "    'task_name': 'go_emotions',\n",
    "    'label_text': label_text\n",
    "})\n",
    "\n",
    "emo['hypothesis'].replace({'This text expresses neutral.': 'This text does not express emotion.'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57cf0e-bbbb-41d1-a48d-6fb29e494869",
   "metadata": {},
   "source": [
    "# Combine and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "392d546a-bb21-4ec8-8c80-6dbfcaeb65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(\n",
    "    [dataset.to_pandas() for dataset in zsnli.values()],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9b0df34a-3a56-4aac-94d3-fdc63826e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hypoths = {\n",
    "'The sentiment in this example rotten tomatoes movie review is positive': 'The sentiment of this review is positive.',\n",
    "'The sentiment in this example rotten tomatoes movie review is negative': 'The sentiment of this review is negative.',\n",
    "'The sentiment in this example amazon product review is positive': 'The sentiment of this review is positive.',\n",
    "'The sentiment in this example amazon product review is negative': 'The sentiment of this review is negative.',\n",
    "'This example question from the Yahoo Q&A forum is categorized in the topic: Computers & Internet': 'This text is about computers & internet.',\n",
    "'This example question from the Yahoo Q&A forum is categorized in the topic: Sports': 'This text is about sports.',\n",
    "'This example question from the Yahoo Q&A forum is categorized in the topic: Family & Relationships': 'This text is about family & relationships.',\n",
    "'This example question from the Yahoo Q&A forum is categorized in the topic: Education & Reference': 'This text is about education and reference.',\n",
    "'This example question from the Yahoo Q&A forum is categorized in the topic: Entertainment & Music': 'This text is about entertainment and music.',\n",
    "'This example question from the Yahoo Q&A forum is categorized in the topic: Health': 'This text is about health.',\n",
    "'This example question from the Yahoo Q&A forum is categorized in the topic: Society & Culture': 'This text is about society & culture.',\n",
    "'This example question from the Yahoo Q&A forum is categorized in the topic: Science & Mathematics': 'This text is about science & mathematics.',\n",
    "'This example question from the Yahoo Q&A forum is categorized in the topic: Politics & Government': 'This text is about politics & government.',\n",
    "'This example question from the Yahoo Q&A forum is categorized in the topic: Business & Finance': 'This text is about business & finance.',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a589fc9a-321d-4f3a-b873-b70b13138752",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['hypothesis'].replace(new_hypoths, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a98b2e4b-0f6c-46d6-a074-16451250d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('../data/out_domain_bench.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fe31e12f-021b-4b99-9e74-97dd2a75378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_nli = pd.concat(\n",
    "    [dataset.to_pandas() for dataset in nli.values()],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "83ef0e52-49d4-4a2a-b22c-d9ea1aa56e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_nli.to_csv('../data/nli_bench.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce5adb-10e6-4d0b-8f67-d8ab38bd67ec",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72a4aa10-c1c2-4b45-bd3e-42124028ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and format hypotheses for prompt\n",
    "zsnli = pd.read_csv('../data/out_domain_bench.csv')\n",
    "zsnli['hypothesis'] = zsnli['hypothesis'].str.lower()\n",
    "zsnli['hypothesis'] = zsnli['hypothesis'].str.replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e7682db-4967-4c5f-81fb-c974610c8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = open('../openAI_key.txt', 'r').read()\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53fcc7d9-a9d9-48ec-9f69-9ea605957429",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a text classifier and are only allowed to respond with 0 or 1.\"\n",
    "\n",
    "user_message = \"\"\"You are a classifier that determines if {hypothesis}. If it is true that {hypothesis}, return 0. If it is not true that {hypothesis} or it cannot be determined, return 1. Do not explain your reasoning. Only respond with 0 or 1.\n",
    "\n",
    "Here is the document:\\n{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29d89b95-85ed-414d-88d9-4b364db30b5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dictionary of requests\n",
    "data = zsnli\n",
    "messages_batch = [\n",
    "    {\n",
    "        'custom_id': str(i),\n",
    "        'method': 'POST',\n",
    "        'url': '/v1/chat/completions',\n",
    "        'body':{\n",
    "        'model': 'gpt-4o-2024-11-20',\n",
    "        'messages': [\n",
    "            {'role': 'system', 'content': system_message},\n",
    "            {'role': 'user', 'content': user_message.format(text=data.loc[i, 'text'], hypothesis = data.loc[i, 'hypothesis'])}\n",
    "        ],\n",
    "        'max_tokens': 1,\n",
    "        'temperature': 0,\n",
    "        'logit_bias': {16: 100, 15: 100}\n",
    "    }\n",
    "    }\n",
    "    for i in data.index\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "239e3b9c-e2c4-43dc-b5f4-7e9ed2d4fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as .jsonl file\n",
    "with open('../data/out_domain_bench.jsonl', 'w') as outfile:\n",
    "    for entry in messages_batch:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a78e0d4-6a47-4360-86b6-a1b017778723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-PhDbCmoUi99YZcR1MENAQy', bytes=10747287, created_at=1735597417, filename='out_domain_bench.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.files.create(\n",
    "  file=open('../data/out_domain_bench.jsonl', \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e712409-b007-45c7-aa30-7e529ce5fe0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(id='batch_67731d923cb8819091c0af22a16f994e', completion_window='24h', created_at=1735597458, endpoint='/v1/chat/completions', input_file_id='file-PhDbCmoUi99YZcR1MENAQy', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1735683858, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.batches.create(\n",
    "  input_file_id=\"file-PhDbCmoUi99YZcR1MENAQy\",\n",
    "  endpoint=\"/v1/chat/completions\",\n",
    "  completion_window=\"24h\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac5739-e5d5-416b-9a60-3fc5500fd2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the completed batch\n",
    "res = pd.read_json(path_or_buf='../data/OOS_batch_output.jsonl', lines=True)\n",
    "labels = [res['response'][i]['body']['choices'][0]['message']['content'] for i in res.index]\n",
    "\n",
    "zsnli['validated_labels'] = labels\n",
    "\n",
    "zsnli['validated_labels'] = [int(label) for label in zsnli['validated_labels']]\n",
    "\n",
    "validated = zsnli[zsnli['labels'] == zsnli['validated_labels']]\n",
    "\n",
    "validated['validation_source'] = 'gpt-4o-2024-11-20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f73629fc-d1f4-4eea-b906-d810ab4556f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validated.to_csv('../data/out_domain_bench.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
