{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cfa482f-2ce5-4cb7-a1f5-c2da1aca32ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikeb\\miniforge3\\envs\\sandbox\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27fac9a3-0c1d-4638-8660-00d6c958fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(df, preds, group_by=None):\n",
    "    \"\"\"\n",
    "    Calculate MCC, Accuracy, F1 for predictions.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing true and predicted labels.\n",
    "        preds (list): List of column names containing model predictions.\n",
    "        group_by (str, optional): Column name to group by ('dataset' or 'task'). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with calculated metrics, optionally grouped by `group_by`.\n",
    "    \"\"\"\n",
    "    true_col = 'entailment'\n",
    "    \n",
    "    def get_metrics(y_true, y_pred):\n",
    "        return {\n",
    "            'MCC': matthews_corrcoef(y_true, y_pred),\n",
    "            'Accuracy': accuracy_score(y_true, y_pred),\n",
    "            'F1': f1_score(y_true, y_pred, average='macro')\n",
    "        }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if group_by not in ['dataset', 'task']:\n",
    "        for col in preds:\n",
    "            metrics = get_metrics(df[true_col], df[col])\n",
    "            metrics['Column'] = col\n",
    "            results.append(metrics)\n",
    "    else:\n",
    "        for col in preds:\n",
    "            for group_name, group in df.groupby(group_by):\n",
    "                metrics = get_metrics(group[true_col], group[col])\n",
    "                metrics['Column'] = col\n",
    "                metrics[group_by.capitalize()] = group_name\n",
    "                results.append(metrics)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if group_by in ['dataset', 'task']:\n",
    "        return results_df.set_index(['Column', group_by.capitalize()])\n",
    "    else:\n",
    "        return results_df.set_index('Column')\n",
    "\n",
    "def bootstrapped_errors(y_true, y_pred, n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Calculate bootstrapped standard errors for MCC, Accuracy, and F1.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        n_bootstrap (int, optional): Number of bootstrap samples. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        dict: Standard errors for MCC, Accuracy, and F1.\n",
    "    \"\"\"\n",
    "    mcc_scores = []\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        y_true_resampled, y_pred_resampled = resample(y_true, y_pred)\n",
    "        \n",
    "        # Calculate metrics for the resampled data\n",
    "        mcc_scores.append(matthews_corrcoef(y_true_resampled, y_pred_resampled))\n",
    "        accuracy_scores.append(accuracy_score(y_true_resampled, y_pred_resampled))\n",
    "        f1_scores.append(f1_score(y_true_resampled, y_pred_resampled, average='weighted'))\n",
    "    \n",
    "    # Calculate standard errors\n",
    "    return {\n",
    "        'MCC_SE': np.std(mcc_scores),\n",
    "        'Accuracy_SE': np.std(accuracy_scores),\n",
    "        'F1_SE': np.std(f1_scores)\n",
    "    }\n",
    "\n",
    "def metrics_with_errors(df, preds, n_bootstrap=1000, group_by=None):\n",
    "    \"\"\"\n",
    "    Calculate metrics and bootstrapped standard errors for predictions, optionally grouped.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing true and predicted labels.\n",
    "        preds (list): List of column names containing model predictions.\n",
    "        n_bootstrap (int, optional): Number of bootstrap samples. Defaults to 1000.\n",
    "        group_by (str, optional): Column name to group by ('dataset' or 'task'). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of metrics, standard errors, and confidence intervals.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate metrics for each model\n",
    "    metrics_df = metrics(df, preds, group_by=group_by)\n",
    "\n",
    "    # Step 2: Calculate bootstrapped errors for each model or group\n",
    "    errors = []\n",
    "    if group_by not in ['dataset', 'task']:\n",
    "        for col in preds:\n",
    "            y_true = df['entailment']\n",
    "            y_pred = df[col]\n",
    "            errors_dict = bootstrapped_errors(y_true, y_pred, n_bootstrap=n_bootstrap)\n",
    "            errors_dict['Column'] = col\n",
    "            errors.append(errors_dict)\n",
    "    else:\n",
    "        for col in preds:\n",
    "            for group_name, group in df.groupby(group_by):\n",
    "                y_true = group['entailment']\n",
    "                y_pred = group[col]\n",
    "                errors_dict = bootstrapped_errors(y_true, y_pred, n_bootstrap=n_bootstrap)\n",
    "                errors_dict['Column'] = col\n",
    "                errors_dict[group_by.capitalize()] = group_name\n",
    "                errors.append(errors_dict)\n",
    "\n",
    "    errors_df = pd.DataFrame(errors)\n",
    "\n",
    "    if group_by in ['dataset', 'task']:\n",
    "        errors_df = errors_df.set_index(['Column', group_by.capitalize()])\n",
    "    else:\n",
    "        errors_df = errors_df.set_index('Column')\n",
    "\n",
    "    # Step 3: Merge metrics and errors DataFrames\n",
    "    combined_df = metrics_df.merge(errors_df, left_index=True, right_index=True)\n",
    "\n",
    "    # Step 4: Calculate confidence intervals (upper and lower bounds)\n",
    "    combined_df['MCC_Lower'] = combined_df['MCC'] - combined_df['MCC_SE']\n",
    "    combined_df['MCC_Upper'] = combined_df['MCC'] + combined_df['MCC_SE']\n",
    "\n",
    "    combined_df['Accuracy_Lower'] = combined_df['Accuracy'] - combined_df['Accuracy_SE']\n",
    "    combined_df['Accuracy_Upper'] = combined_df['Accuracy'] + combined_df['Accuracy_SE']\n",
    "\n",
    "    combined_df['F1_Lower'] = combined_df['F1'] - combined_df['F1_SE']\n",
    "    combined_df['F1_Upper'] = combined_df['F1'] + combined_df['F1_SE']\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "def label_docs(model, docs_dict, batch_size = 8, device = 'cuda'):\n",
    "    \"\"\"\n",
    "    Passes documents through the pipeline. Returns a list of entail, not_entail labels\n",
    "    \"\"\"\n",
    "    pipe = pipeline(task = 'text-classification', model = model, \n",
    "                    batch_size = batch_size, device = device, \n",
    "                    max_length = 512, truncation = True, \n",
    "                    torch_dtype = torch.bfloat16)\n",
    "    res = pipe(docs_dict)\n",
    "    res = [result['label'] for result in res]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38325f48-8d83-4eb2-b726-9f90cae014d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_key = open('../llama_key.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3092a0ba-b0bf-4987-9692-facd0a16124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 4/4 [00:19<00:00,  4.88s/it]\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"cuda\",\n",
    "    token = llama_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22493379-5acd-4680-8e93-6dc0fd51be2c",
   "metadata": {},
   "source": [
    "# UKP Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68d85c58-c484-49a0-bc1a-feb0d35cf60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"You are a classifier that can only respond with 0 or 1. I'm going to show you a short text sample and I want you to determine if {hypothesis}. Here is the text:\n",
    "{doc}\n",
    "\n",
    "If it is true that {hypothesis}, return 0. If it is not true that {hypothesis}, return 1.\n",
    "Do not explain your answer, and only return 0 or 1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00aa9545-828a-4171-aff5-bbb8d1483dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukp = pd.read_csv('ukp_stance.csv')\n",
    "ukp['hypothesis'] = [hypoth.capitalize() for hypoth in ukp['hypothesis']]\n",
    "ukp.rename({'sentence':'text', 'topic': 'task'}, axis = 1, inplace = True)\n",
    "ukp = ukp[['text', 'entailment', 'hypothesis']]\n",
    "ukp['task'] = 'stance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95b4fa-8aab-4bdf-b3d4-4c906e75b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukp['hypothesis'] = ukp['hypothesis'].str.lower()\n",
    "\n",
    "ukp['hypothesis'] = [hyp[0:-1] for hyp in ukp['hypothesis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1a35fc5-38b6-4415-94ae-60e913814d49",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikeb\\miniforge3\\envs\\sandbox\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mikeb\\miniforge3\\envs\\sandbox\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mikeb\\miniforge3\\envs\\sandbox\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:53: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1', '0'}\n",
      "CPU times: total: 5min 14s\n",
      "Wall time: 7min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = ukp\n",
    "res = []\n",
    "for i in data.index:\n",
    "    doc = data.loc[i, 'text']\n",
    "    hypothesis = data.loc[i, 'hypothesis']\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message.format(doc = doc, hypothesis = hypothesis)},\n",
    "    ]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=2, do_sample=False, return_full_text = False, pad_token_id=pipe.tokenizer.eos_token_id, temperature = 0)\n",
    "    res.extend(outputs)\n",
    "\n",
    "res = [text['generated_text'] for text in res]\n",
    "# return a list of unique responses from the model\n",
    "print(set(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22591c76-c7be-4d40-8104-c32cc6e12246",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = [num[0] for num in res]\n",
    "ukp['llama'] = [1 if '1' in text else 0 for text in labs]\n",
    "\n",
    "overall = metrics_with_errors(ukp, ['llama'], group_by = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6515826d-7023-4b93-848c-4972d09912ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>MCC_SE</th>\n",
       "      <th>Accuracy_SE</th>\n",
       "      <th>F1_SE</th>\n",
       "      <th>MCC_Lower</th>\n",
       "      <th>MCC_Upper</th>\n",
       "      <th>Accuracy_Lower</th>\n",
       "      <th>Accuracy_Upper</th>\n",
       "      <th>F1_Lower</th>\n",
       "      <th>F1_Upper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama</th>\n",
       "      <td>0.764781</td>\n",
       "      <td>0.897154</td>\n",
       "      <td>0.882336</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.00425</td>\n",
       "      <td>0.755006</td>\n",
       "      <td>0.774555</td>\n",
       "      <td>0.892882</td>\n",
       "      <td>0.901427</td>\n",
       "      <td>0.878086</td>\n",
       "      <td>0.886586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MCC  Accuracy        F1    MCC_SE  Accuracy_SE    F1_SE  \\\n",
       "Column                                                                 \n",
       "llama   0.764781  0.897154  0.882336  0.009774     0.004273  0.00425   \n",
       "\n",
       "        MCC_Lower  MCC_Upper  Accuracy_Lower  Accuracy_Upper  F1_Lower  \\\n",
       "Column                                                                   \n",
       "llama    0.755006   0.774555        0.892882        0.901427  0.878086   \n",
       "\n",
       "        F1_Upper  \n",
       "Column            \n",
       "llama   0.886586  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "741a2142-6789-42c8-a2ba-ce01cee0d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukp.to_csv('../data/ukp_stance.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5299758-e786-49ca-852f-4f5dc15b1631",
   "metadata": {},
   "source": [
    "# UKP Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d620454-8c12-4005-ba7c-dfe99e2d2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"You are a classifier that can only respond with 0 or 1. I'm going to show you a short text sample and I want you to determine if {hypothesis}. Here is the text:\n",
    "{doc}\n",
    "\n",
    "If it is true that {hypothesis}, return 0. If it is not true that {hypothesis}, return 1.\n",
    "Do not explain your answer, and only return 0 or 1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a1958-25df-4711-abee-783d79f4b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = pd.read_csv('../data/ukp_topic.csv')\n",
    "# format hypotheses\n",
    "topic['hypothesis'] = topic['hypothesis'].str.lower()\n",
    "topic['hypothesis'] = [hyp[0:-1] for hyp in topic['hypothesis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8fa21e9-27a4-4494-b2d5-b0f34d9433b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikeb\\miniforge3\\envs\\sandbox\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mikeb\\miniforge3\\envs\\sandbox\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1', '0'}\n",
      "CPU times: total: 10min 9s\n",
      "Wall time: 14min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = topic\n",
    "res = []\n",
    "for i in data.index:\n",
    "    doc = data.loc[i, 'text']\n",
    "    hypothesis = data.loc[i, 'hypothesis']\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message.format(doc = doc, hypothesis = hypothesis)},\n",
    "    ]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=2, do_sample=False, return_full_text = False, pad_token_id=pipe.tokenizer.eos_token_id, temperature = 0)\n",
    "    res.extend(outputs)\n",
    "\n",
    "res = [text['generated_text'] for text in res]\n",
    "# return a list of unique responses from the model\n",
    "print(set(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ecc25ff-6385-47a5-ad6d-d7d29167d46b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labs = [num[0] for num in res]\n",
    "topic['llama'] = [1 if '1' in text else 0 for text in labs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8edd746-a3f7-4239-a4ea-351ab601b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = metrics_with_errors(topic, ['llama'], group_by = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b97ad8b9-2237-4d37-a264-4e509cddecbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>MCC_SE</th>\n",
       "      <th>Accuracy_SE</th>\n",
       "      <th>F1_SE</th>\n",
       "      <th>MCC_Lower</th>\n",
       "      <th>MCC_Upper</th>\n",
       "      <th>Accuracy_Lower</th>\n",
       "      <th>Accuracy_Upper</th>\n",
       "      <th>F1_Lower</th>\n",
       "      <th>F1_Upper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama</th>\n",
       "      <td>0.891902</td>\n",
       "      <td>0.945935</td>\n",
       "      <td>0.945934</td>\n",
       "      <td>0.004567</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.887336</td>\n",
       "      <td>0.896469</td>\n",
       "      <td>0.94365</td>\n",
       "      <td>0.948219</td>\n",
       "      <td>0.943649</td>\n",
       "      <td>0.948219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MCC  Accuracy        F1    MCC_SE  Accuracy_SE     F1_SE  \\\n",
       "Column                                                                  \n",
       "llama   0.891902  0.945935  0.945934  0.004567     0.002285  0.002285   \n",
       "\n",
       "        MCC_Lower  MCC_Upper  Accuracy_Lower  Accuracy_Upper  F1_Lower  \\\n",
       "Column                                                                   \n",
       "llama    0.887336   0.896469         0.94365        0.948219  0.943649   \n",
       "\n",
       "        F1_Upper  \n",
       "Column            \n",
       "llama   0.948219  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0bf71b5f-311d-4906-9640-e7b4a7ae7445",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic.to_csv('../data/ukp_topic.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f56c2-f090-4f20-a40d-898ad378495d",
   "metadata": {},
   "source": [
    "# RAND Terrorism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18e79f39-d712-4813-98dd-334154683369",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"You are a classifier that can only respond with 1, 2, 3, or 4. I'm going to show you a short text sample about a terrorist attack and I want you to determine which of the following labels best describes the text:\n",
    "\n",
    "1: Explosives attack.\n",
    "2: Firearms attack.\n",
    "3: Arson attack.\n",
    "4: Knife or sharp object attack.\n",
    "\n",
    "Return the number of the statement the best describes the text.Here is the text:\n",
    "\n",
    "{doc}\n",
    "\n",
    "Only return the number of the statement that best describes the text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34b457a0-34d4-4cae-98e9-c27bf9707734",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = pd.read_csv('../data/rand_terror.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c2807bf2-672e-4ca4-ac8d-e30548b0f1cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikeb\\miniforge3\\envs\\sandbox\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mikeb\\miniforge3\\envs\\sandbox\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3', '1', '2', '4'}\n",
      "CPU times: total: 2min 12s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = rand\n",
    "res = []\n",
    "for i in data.index:\n",
    "    doc = data.loc[i, 'premise']\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message.format(doc = doc)},\n",
    "    ]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=2, do_sample=False, return_full_text = False, pad_token_id=pipe.tokenizer.eos_token_id, temperature = 0)\n",
    "    res.extend(outputs)\n",
    "\n",
    "res = [text['generated_text'] for text in res]\n",
    "# return a list of unique responses from the model\n",
    "print(set(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0580c810-8470-41a5-ba19-200933687faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = [num[0] for num in res]\n",
    "rand['llama'] = [int(text) for text in labs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "379cc682-2f36-426e-ab85-7cd74d3064ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand['entailment'] = rand['hypothesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0bfcc88-ca02-40e3-aba7-c71956e055be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikeb\\AppData\\Local\\Temp\\ipykernel_24392\\1242171616.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  rand['entailment'].replace({'This text describes an explosives attack.': 1,\n",
      "C:\\Users\\mikeb\\AppData\\Local\\Temp\\ipykernel_24392\\1242171616.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  rand['entailment'].replace({'This text describes an explosives attack.': 1,\n"
     ]
    }
   ],
   "source": [
    "rand['entailment'].replace({'This text describes an explosives attack.': 1,\n",
    "       'This text describes a firearms attack.': 2,\n",
    "       'This text describes an arson attack.': 3,\n",
    "       'This text describes a knife or sharp object attack.': 4}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b0b1dc2-07f5-4896-a0d7-dbc339e8f51a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overall = metrics_with_errors(rand, ['llama'], group_by = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f24af1ab-e678-43e7-a4eb-9a9533cbd4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand.to_csv('../data/rand_terror.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
