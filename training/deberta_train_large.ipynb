{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58833a02-0bd6-4414-b130-225bd714d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, accuracy_score, classification_report, matthews_corrcoef\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387c8db-8ca3-4abc-91ac-f16f0ebe9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"DEBATE-DeBERTa-Large\"\n",
    "\n",
    "modname = \"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\"\n",
    "training_directory ='training_large'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c1008-b5e4-46b8-9cc4-bedb87778c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"mlburnham/Pol_NLI\")\n",
    "\n",
    "df = ds['train'].to_pandas()\n",
    "dftest = ds['test'].to_pandas()\n",
    "dfval = ds['validation'].to_pandas()\n",
    "\n",
    "def truncate(text):\n",
    "    words = text.split()\n",
    "    if len(words) > 450:\n",
    "        return \" \".join(words[:450])\n",
    "    return text\n",
    "\n",
    "df['premise'] = df['premise'].apply(truncate)\n",
    "dftest['premise'] = dftest['premise'].apply(truncate)\n",
    "dfval['premise'] = dfval['premise'].apply(truncate)\n",
    "\n",
    "ds = DatasetDict({'train': Dataset.from_pandas(df, preserve_index=False), 'validation':Dataset.from_pandas(dfval, preserve_index=False), 'test':Dataset.from_pandas(dftest, preserve_index=False)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b957755c-05bb-4a11-b58e-8644b8af43ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9c616445e647a58d14d82ae57a720b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/201691 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f44aa1e21d746fb83c07305c616165d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15036 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd342f51b1c2411688fd2b7ff66de126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15366 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(modname)\n",
    "id2label = {0: \"entailment\", 1: \"not_entailment\"}\n",
    "\n",
    "def tokenize_function(docs):\n",
    "    return tokenizer(docs['premise'], docs['augmented_hypothesis'], padding = True, truncation = True)\n",
    "\n",
    "def model_init():\n",
    "  return AutoModelForSequenceClassification.from_pretrained(modname, \n",
    "                                                           num_labels=2,\n",
    "                                                           ignore_mismatched_sizes=True,\n",
    "                                                           id2label = id2label)\n",
    "\n",
    "dstok = ds.map(tokenize_function, batched = True)\n",
    "dstok = dstok.rename_columns({'entailment':'label'})\n",
    "\n",
    "training_args = TrainingArguments(output_dir=training_directory,\n",
    "    logging_dir=f'{training_directory}/logs',\n",
    "    lr_scheduler_type= \"linear\",\n",
    "    group_by_length=True,\n",
    "    learning_rate=9e-6 if \"large\" in modname else 2e-5,\n",
    "    per_device_train_batch_size=4 if \"large\" in modname else 16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4 if \"large\" in modname else 1,  \n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    seed=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    dataloader_num_workers = 12,\n",
    ")\n",
    "\n",
    "def compute_metrics_standard(eval_pred, label_text_alphabetical=list(id2label.values())):\n",
    "    labels = eval_pred.label_ids\n",
    "    pred_logits = eval_pred.predictions\n",
    "    preds_max = np.argmax(pred_logits, axis=1)\n",
    "\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels, preds_max, average='macro')\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(labels, preds_max, average='micro')\n",
    "    acc_balanced = balanced_accuracy_score(labels, preds_max)\n",
    "    acc_not_balanced = accuracy_score(labels, preds_max)\n",
    "    mcc = matthews_corrcoef(labels, preds_max)\n",
    "\n",
    "    metrics = {'MCC': mcc,\n",
    "            'f1_macro': f1_macro,\n",
    "            'f1_micro': f1_micro,\n",
    "            'accuracy_balanced': acc_balanced,\n",
    "            'accuracy': acc_not_balanced,\n",
    "            'precision_macro': precision_macro,\n",
    "            'recall_macro': recall_macro,\n",
    "            'precision_micro': precision_micro,\n",
    "            'recall_micro': recall_micro,\n",
    "            }\n",
    "    print(\"Aggregate metrics: \", {key: metrics[key] for key in metrics if key not in [\"label_gold_raw\", \"label_predicted_raw\"]} )\n",
    "    print(\"Detailed metrics: \", classification_report(\n",
    "        labels, preds_max, labels=np.sort(pd.factorize(label_text_alphabetical, sort=True)[0]),\n",
    "        target_names=label_text_alphabetical, sample_weight=None,\n",
    "        digits=2, output_dict=True, zero_division='warn'),\n",
    "    \"\\n\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dstok['train'],\n",
    "    eval_dataset=dstok['validation'],\n",
    "    compute_metrics=lambda x: compute_metrics_standard(x, label_text_alphabetical=list(id2label.values()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248b994-41f1-4615-856c-bef3ae94e831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmlburnham\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mike/modernbert/wandb/run-20250108_175242-m2vzl8ca</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlburnham/DEBATE-debertalarge/runs/m2vzl8ca' target=\"_blank\">training_large</a></strong> to <a href='https://wandb.ai/mlburnham/DEBATE-debertalarge' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlburnham/DEBATE-debertalarge' target=\"_blank\">https://wandb.ai/mlburnham/DEBATE-debertalarge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlburnham/DEBATE-debertalarge/runs/m2vzl8ca' target=\"_blank\">https://wandb.ai/mlburnham/DEBATE-debertalarge/runs/m2vzl8ca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5547' max='126050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5547/126050 1:17:35 < 28:06:05, 1.19 it/s, Epoch 0.44/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
